Log file created at 20:12:52 01-23 with path: logs/01.23-20:12:49.log
System information:
+-----------------------+------------------------------------+
|    CPU Physical Cores |                                 16 |
+-----------------------+------------------------------------+
|     CPU Logical Cores |                                 32 |
+-----------------------+------------------------------------+
|     CPU Max Frequency |                           0.00 MHz |
+-----------------------+------------------------------------+
| CPU Current Frequency |                        2419.20 MHz |
+-----------------------+------------------------------------+
|             RAM Total |                           10.70 GB |
+-----------------------+------------------------------------+
|         RAM Available |                            7.56 GB |
+-----------------------+------------------------------------+
|              RAM Used |                            2.86 GB |
+-----------------------+------------------------------------+
|            GPU Device | NVIDIA GeForce RTX 4060 Laptop GPU |
+-----------------------+------------------------------------+
|             GPU Count |                                  1 |
+-----------------------+------------------------------------+
|      GPU Memory Total |                            8.00 GB |
+-----------------------+------------------------------------+
|   GPU Memory Reserved |                            0.00 GB |
+-----------------------+------------------------------------+
|          CUDA Version |                               12.4 |
+-----------------------+------------------------------------+
|              Hostname |                            AmadFat |
+-----------------------+------------------------------------+
|                    OS |                              Linux |
+-----------------------+------------------------------------+
|            OS Release | 5.15.167.4-microsoft-standard-WSL2 |
+-----------------------+------------------------------------+
|            OS Version | #1 SMP Tue Nov 5 00:21:55 UTC 2024 |
+-----------------------+------------------------------------+
|        Python Version |                             3.9.21 |
+-----------------------+------------------------------------+
|       PyTorch Version |                        2.5.1+cu124 |
+-----------------------+------------------------------------+
|   TorchVision Version |                       0.20.1+cu124 |
+-----------------------+------------------------------------+
|    TorchAudio Version |                        2.5.1+cu124 |
+-----------------------+------------------------------------+
Experiment settings:
{'criterion': {'criterion_name': 'celoss', 'label_smoothing': 0.2},
 'data': {'batch_size': 2,
          'num_workers': 0,
          'root': 'dataset',
          'split_ratio': [0.8, 0.2]},
 'evaluator': {'acc': True, 'loss': True},
 'experiment': {'ckpt_save_dir': PosixPath('ckpts/01.23-20:12:49'),
                'deterministic': True,
                'device': 'cpu',
                'experiment_name': '01.23-20:12:49',
                'max_epochs': 20,
                'save': True,
                'seed': 3407,
                'val': True,
                'val_interval': 1},
 'logger': {'log_save_path': PosixPath('logs/01.23-20:12:49.log'),
            'tb_save_path': PosixPath('tbevents/01.23-20:12:49'),
            'train_print_interval': 20,
            'use_tensorboard': True,
            'window_metric': 20,
            'window_time_stamp': 20},
 'model': {'dropout': 0.2,
           'last_fc_bias_init': 'const',
           'last_fc_weight_init': 'const',
           'model_name': 'mobilenet_v3_s',
           'norm_layer': 'batchnorm',
           'weights': 'default'},
 'optimizer': {'grad_clip': 1.0,
               'lr': 0.001,
               'momentum': 0.9,
               'optimizer_name': 'sgd',
               'weight_decay': 5e-05},
 'scheduler': {'milestones': [5, 10], 'scheduler_name': 'steplr'},
 'transform': {'train': {'compose': {'transforms': [{'autoaugment': {'interpolation': 'nearest',
                                                                     'policy': 'imagenet'}},
                                                    {'totensor': {}}]}},
               'val': {'totensor': {}}}}
20:12:59 01-23 [epoch 1 train] [step 1/386] [eta N/A]: lr 0.0010(0.0010) | loss 2.0794(2.0794)
20:13:01 01-23 [epoch 1 train] [step 20/386] [eta 0:00:38]: lr 0.0010(0.0010) | loss 2.0770(2.0770)
20:13:03 01-23 [epoch 1 train] [step 40/386] [eta 0:00:36]: lr 0.0010(0.0010) | loss 2.0652(2.0533)
20:13:05 01-23 [epoch 1 train] [step 60/386] [eta 0:00:35]: lr 0.0010(0.0010) | loss 2.0613(2.0535)
20:13:07 01-23 [epoch 1 train] [step 80/386] [eta 0:00:31]: lr 0.0010(0.0010) | loss 2.0581(2.0485)
20:13:09 01-23 [epoch 1 train] [step 100/386] [eta 0:00:29]: lr 0.0010(0.0010) | loss 2.0413(1.9740)
20:13:11 01-23 [epoch 1 train] [step 120/386] [eta 0:00:27]: lr 0.0010(0.0010) | loss 2.0455(2.0668)
20:13:13 01-23 [epoch 1 train] [step 140/386] [eta 0:00:26]: lr 0.0010(0.0010) | loss 2.0473(2.0578)
20:13:39 01-23 [epoch 1 train] [step 160/386] [eta 0:00:23]: lr 0.0010(0.0010) | loss 2.0450(2.0294)
20:13:17 01-23 [epoch 1 train] [step 180/386] [eta 0:00:22]: lr 0.0010(0.0010) | loss 2.0494(2.0846)
20:13:20 01-23 [epoch 1 train] [step 200/386] [eta 0:00:20]: lr 0.0010(0.0010) | loss 2.0513(2.0679)
20:13:22 01-23 [epoch 1 train] [step 220/386] [eta 0:00:18]: lr 0.0010(0.0010) | loss 2.0490(2.0258)
20:13:24 01-23 [epoch 1 train] [step 240/386] [eta 0:00:15]: lr 0.0010(0.0010) | loss 2.0495(2.0548)
20:13:26 01-23 [epoch 1 train] [step 260/386] [eta 0:00:12]: lr 0.0010(0.0010) | loss 2.0447(1.9880)
20:13:28 01-23 [epoch 1 train] [step 280/386] [eta 0:00:12]: lr 0.0010(0.0010) | loss 2.0394(1.9704)
20:13:54 01-23 [epoch 1 train] [step 300/386] [eta 0:00:09]: lr 0.0010(0.0010) | loss 2.0352(1.9760)
20:13:33 01-23 [epoch 1 train] [step 320/386] [eta 0:00:07]: lr 0.0010(0.0010) | loss 2.0362(2.0508)
20:13:35 01-23 [epoch 1 train] [step 340/386] [eta 0:00:04]: lr 0.0010(0.0010) | loss 2.0312(1.9522)
20:13:37 01-23 [epoch 1 train] [step 360/386] [eta 0:00:02]: lr 0.0010(0.0010) | loss 2.0285(1.9824)
20:13:39 01-23 [epoch 1 train] [step 380/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 2.0282(2.0234)
20:13:40 01-23 [epoch 1 train] [step 386/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 2.0260(1.9689)
20:13:53 01-23 [epoch 1 val]: Accuracy 29.6875 | Loss 1.9908
20:13:53 01-23 [epoch 2 train] [step 1/386] [eta 0:05:05]: lr 0.0010(0.0010) | loss 2.0555(1.9669)
20:13:56 01-23 [epoch 2 train] [step 20/386] [eta 0:04:54]: lr 0.0010(0.0010) | loss 1.9932(1.9932)
20:13:58 01-23 [epoch 2 train] [step 40/386] [eta 0:00:41]: lr 0.0010(0.0010) | loss 1.9719(1.9506)
20:14:00 01-23 [epoch 2 train] [step 60/386] [eta 0:00:34]: lr 0.0010(0.0010) | loss 1.9481(1.9005)
20:14:02 01-23 [epoch 2 train] [step 80/386] [eta 0:00:34]: lr 0.0010(0.0010) | loss 1.9742(2.0523)
20:14:04 01-23 [epoch 2 train] [step 100/386] [eta 0:00:28]: lr 0.0010(0.0010) | loss 1.9845(2.0258)
20:14:06 01-23 [epoch 2 train] [step 120/386] [eta 0:00:26]: lr 0.0010(0.0010) | loss 1.9826(1.9733)
20:14:09 01-23 [epoch 2 train] [step 140/386] [eta 0:00:25]: lr 0.0010(0.0010) | loss 1.9740(1.9223)
20:14:34 01-23 [epoch 2 train] [step 160/386] [eta 0:00:24]: lr 0.0010(0.0010) | loss 1.9756(1.9863)
20:14:13 01-23 [epoch 2 train] [step 180/386] [eta 0:00:20]: lr 0.0010(0.0010) | loss 1.9763(1.9827)
20:14:15 01-23 [epoch 2 train] [step 200/386] [eta 0:00:19]: lr 0.0010(0.0010) | loss 1.9845(2.0574)
20:14:17 01-23 [epoch 2 train] [step 220/386] [eta 0:00:16]: lr 0.0010(0.0010) | loss 1.9810(1.9461)
20:14:19 01-23 [epoch 2 train] [step 240/386] [eta 0:00:14]: lr 0.0010(0.0010) | loss 1.9811(1.9825)
20:14:21 01-23 [epoch 2 train] [step 260/386] [eta 0:00:12]: lr 0.0010(0.0010) | loss 1.9816(1.9872)
20:14:23 01-23 [epoch 2 train] [step 280/386] [eta 0:00:10]: lr 0.0010(0.0010) | loss 1.9772(1.9204)
20:14:25 01-23 [epoch 2 train] [step 300/386] [eta 0:00:09]: lr 0.0010(0.0010) | loss 1.9738(1.9263)
20:14:27 01-23 [epoch 2 train] [step 320/386] [eta 0:00:06]: lr 0.0010(0.0010) | loss 1.9716(1.9393)
20:14:29 01-23 [epoch 2 train] [step 340/386] [eta 0:00:04]: lr 0.0010(0.0010) | loss 1.9694(1.9340)
20:14:31 01-23 [epoch 2 train] [step 360/386] [eta 0:00:02]: lr 0.0010(0.0010) | loss 1.9715(2.0073)
20:14:33 01-23 [epoch 2 train] [step 380/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.9723(1.9862)
20:14:34 01-23 [epoch 2 train] [step 386/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.9714(1.9407)
20:15:11 01-23 [epoch 2 val]: Accuracy 41.6667 | Loss 1.9425
20:15:12 01-23 [epoch 3 train] [step 1/386] [eta 0:05:14]: lr 0.0010(0.0010) | loss 1.6055(1.9104)
20:15:14 01-23 [epoch 3 train] [step 20/386] [eta 0:04:59]: lr 0.0010(0.0010) | loss 1.8924(1.8924)
20:14:53 01-23 [epoch 3 train] [step 40/386] [eta 0:00:37]: lr 0.0010(0.0010) | loss 1.8996(1.9068)
20:14:55 01-23 [epoch 3 train] [step 60/386] [eta 0:00:36]: lr 0.0010(0.0010) | loss 1.8855(1.8573)
20:14:57 01-23 [epoch 3 train] [step 80/386] [eta 0:00:32]: lr 0.0010(0.0010) | loss 1.8918(1.9106)
20:14:59 01-23 [epoch 3 train] [step 100/386] [eta 0:00:29]: lr 0.0010(0.0010) | loss 1.8878(1.8719)
20:15:01 01-23 [epoch 3 train] [step 120/386] [eta 0:00:27]: lr 0.0010(0.0010) | loss 1.8894(1.8972)
20:15:03 01-23 [epoch 3 train] [step 140/386] [eta 0:00:23]: lr 0.0010(0.0010) | loss 1.8916(1.9053)
20:15:05 01-23 [epoch 3 train] [step 160/386] [eta 0:00:22]: lr 0.0010(0.0010) | loss 1.8955(1.9229)
20:15:07 01-23 [epoch 3 train] [step 180/386] [eta 0:00:22]: lr 0.0010(0.0010) | loss 1.8910(1.8551)
20:15:09 01-23 [epoch 3 train] [step 200/386] [eta 0:00:18]: lr 0.0010(0.0010) | loss 1.8968(1.9485)
20:15:11 01-23 [epoch 3 train] [step 220/386] [eta 0:00:17]: lr 0.0010(0.0010) | loss 1.8980(1.9105)
20:15:14 01-23 [epoch 3 train] [step 240/386] [eta 0:00:15]: lr 0.0010(0.0010) | loss 1.9060(1.9941)
20:15:39 01-23 [epoch 3 train] [step 260/386] [eta 0:00:13]: lr 0.0010(0.0010) | loss 1.9059(1.9049)
20:15:18 01-23 [epoch 3 train] [step 280/386] [eta 0:00:10]: lr 0.0010(0.0010) | loss 1.9021(1.8522)
20:15:20 01-23 [epoch 3 train] [step 300/386] [eta 0:00:08]: lr 0.0010(0.0010) | loss 1.8997(1.8659)
20:15:22 01-23 [epoch 3 train] [step 320/386] [eta 0:00:06]: lr 0.0010(0.0010) | loss 1.9029(1.9505)
20:15:24 01-23 [epoch 3 train] [step 340/386] [eta 0:00:04]: lr 0.0010(0.0010) | loss 1.9045(1.9299)
20:15:26 01-23 [epoch 3 train] [step 360/386] [eta 0:00:02]: lr 0.0010(0.0010) | loss 1.9016(1.8538)
20:15:28 01-23 [epoch 3 train] [step 380/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.9063(1.9905)
20:15:29 01-23 [epoch 3 train] [step 386/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.9034(1.8937)
20:15:42 01-23 [epoch 3 val]: Accuracy 53.6458 | Loss 1.8386
20:15:42 01-23 [epoch 4 train] [step 1/386] [eta 0:04:59]: lr 0.0010(0.0010) | loss 1.7366(1.8835)
20:15:44 01-23 [epoch 4 train] [step 20/386] [eta 0:04:41]: lr 0.0010(0.0010) | loss 1.7999(1.7999)
20:15:46 01-23 [epoch 4 train] [step 40/386] [eta 0:00:34]: lr 0.0010(0.0010) | loss 1.8263(1.8526)
20:15:48 01-23 [epoch 4 train] [step 60/386] [eta 0:00:33]: lr 0.0010(0.0010) | loss 1.8318(1.8430)
20:15:50 01-23 [epoch 4 train] [step 80/386] [eta 0:00:31]: lr 0.0010(0.0010) | loss 1.8497(1.9033)
20:15:52 01-23 [epoch 4 train] [step 100/386] [eta 0:00:28]: lr 0.0010(0.0010) | loss 1.8542(1.8722)
20:15:54 01-23 [epoch 4 train] [step 120/386] [eta 0:00:27]: lr 0.0010(0.0010) | loss 1.8417(1.7790)
20:15:56 01-23 [epoch 4 train] [step 140/386] [eta 0:00:25]: lr 0.0010(0.0010) | loss 1.8300(1.7599)
20:15:59 01-23 [epoch 4 train] [step 160/386] [eta 0:00:24]: lr 0.0010(0.0010) | loss 1.8410(1.9178)
20:16:24 01-23 [epoch 4 train] [step 180/386] [eta 0:00:21]: lr 0.0010(0.0010) | loss 1.8314(1.7551)
20:16:03 01-23 [epoch 4 train] [step 200/386] [eta 0:00:19]: lr 0.0010(0.0010) | loss 1.8217(1.7337)
20:16:05 01-23 [epoch 4 train] [step 220/386] [eta 0:00:16]: lr 0.0010(0.0010) | loss 1.8292(1.9051)
20:16:07 01-23 [epoch 4 train] [step 240/386] [eta 0:00:14]: lr 0.0010(0.0010) | loss 1.8380(1.9342)
20:16:09 01-23 [epoch 4 train] [step 260/386] [eta 0:00:13]: lr 0.0010(0.0010) | loss 1.8379(1.8368)
20:16:11 01-23 [epoch 4 train] [step 280/386] [eta 0:00:11]: lr 0.0010(0.0010) | loss 1.8305(1.7339)
20:16:13 01-23 [epoch 4 train] [step 300/386] [eta 0:00:09]: lr 0.0010(0.0010) | loss 1.8286(1.8032)
20:16:15 01-23 [epoch 4 train] [step 320/386] [eta 0:00:07]: lr 0.0010(0.0010) | loss 1.8245(1.7626)
20:16:17 01-23 [epoch 4 train] [step 340/386] [eta 0:00:04]: lr 0.0010(0.0010) | loss 1.8245(1.8243)
20:16:20 01-23 [epoch 4 train] [step 360/386] [eta 0:00:02]: lr 0.0010(0.0010) | loss 1.8207(1.7560)
20:16:22 01-23 [epoch 4 train] [step 380/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.8243(1.8891)
20:16:22 01-23 [epoch 4 train] [step 386/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.8223(1.8385)
20:16:59 01-23 [epoch 4 val]: Accuracy 58.8542 | Loss 1.7424
20:16:36 01-23 [epoch 5 train] [step 1/386] [eta 0:05:01]: lr 0.0010(0.0010) | loss 1.3431(1.8175)
20:16:38 01-23 [epoch 5 train] [step 20/386] [eta 0:04:48]: lr 0.0010(0.0010) | loss 1.7297(1.7297)
20:16:40 01-23 [epoch 5 train] [step 40/386] [eta 0:00:35]: lr 0.0010(0.0010) | loss 1.7339(1.7380)
20:16:42 01-23 [epoch 5 train] [step 60/386] [eta 0:00:35]: lr 0.0010(0.0010) | loss 1.7410(1.7551)
20:16:44 01-23 [epoch 5 train] [step 80/386] [eta 0:00:32]: lr 0.0010(0.0010) | loss 1.7210(1.6612)
20:16:47 01-23 [epoch 5 train] [step 100/386] [eta 0:00:29]: lr 0.0010(0.0010) | loss 1.7092(1.6618)
20:16:49 01-23 [epoch 5 train] [step 120/386] [eta 0:00:26]: lr 0.0010(0.0010) | loss 1.7143(1.7396)
20:17:14 01-23 [epoch 5 train] [step 140/386] [eta 0:00:26]: lr 0.0010(0.0010) | loss 1.7116(1.6953)
20:16:53 01-23 [epoch 5 train] [step 160/386] [eta 0:00:23]: lr 0.0010(0.0010) | loss 1.7086(1.6883)
20:16:55 01-23 [epoch 5 train] [step 180/386] [eta 0:00:22]: lr 0.0010(0.0010) | loss 1.7091(1.7124)
20:16:57 01-23 [epoch 5 train] [step 200/386] [eta 0:00:18]: lr 0.0010(0.0010) | loss 1.7126(1.7448)
20:16:59 01-23 [epoch 5 train] [step 220/386] [eta 0:00:17]: lr 0.0010(0.0010) | loss 1.7171(1.7622)
20:17:01 01-23 [epoch 5 train] [step 240/386] [eta 0:00:14]: lr 0.0010(0.0010) | loss 1.7205(1.7570)
20:17:03 01-23 [epoch 5 train] [step 260/386] [eta 0:00:12]: lr 0.0010(0.0010) | loss 1.7218(1.7373)
20:17:05 01-23 [epoch 5 train] [step 280/386] [eta 0:00:10]: lr 0.0010(0.0010) | loss 1.7285(1.8159)
20:17:07 01-23 [epoch 5 train] [step 300/386] [eta 0:00:08]: lr 0.0010(0.0010) | loss 1.7320(1.7818)
20:17:09 01-23 [epoch 5 train] [step 320/386] [eta 0:00:06]: lr 0.0010(0.0010) | loss 1.7297(1.6946)
20:17:34 01-23 [epoch 5 train] [step 340/386] [eta 0:00:04]: lr 0.0010(0.0010) | loss 1.7319(1.7668)
20:17:13 01-23 [epoch 5 train] [step 360/386] [eta 0:00:02]: lr 0.0010(0.0010) | loss 1.7322(1.7377)
20:17:15 01-23 [epoch 5 train] [step 380/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.7325(1.7372)
20:17:16 01-23 [epoch 5 train] [step 386/386] [eta 0:00:00]: lr 0.0010(0.0010) | loss 1.7318(1.7137)
20:17:29 01-23 [epoch 5 val]: Accuracy 60.9375 | Loss 1.6112
20:17:29 01-23 [epoch 6 train] [step 1/386] [eta 0:04:52]: lr 0.0003(0.0010) | loss 1.6990(1.7191)
20:17:54 01-23 [epoch 6 train] [step 20/386] [eta 0:04:41]: lr 0.0003(0.0003) | loss 1.8527(1.8527)
20:17:33 01-23 [epoch 6 train] [step 40/386] [eta 0:00:33]: lr 0.0003(0.0003) | loss 1.7121(1.5714)
20:17:35 01-23 [epoch 6 train] [step 60/386] [eta 0:00:31]: lr 0.0003(0.0003) | loss 1.7139(1.7174)
20:17:37 01-23 [epoch 6 train] [step 80/386] [eta 0:00:30]: lr 0.0003(0.0003) | loss 1.7291(1.7747)
20:17:39 01-23 [epoch 6 train] [step 100/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.7313(1.7402)
20:17:41 01-23 [epoch 6 train] [step 120/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.7119(1.6148)
20:17:43 01-23 [epoch 6 train] [step 140/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.7039(1.6559)
20:17:45 01-23 [epoch 6 train] [step 160/386] [eta 0:00:23]: lr 0.0003(0.0003) | loss 1.6917(1.6068)
20:17:47 01-23 [epoch 6 train] [step 180/386] [eta 0:00:21]: lr 0.0003(0.0003) | loss 1.6860(1.6401)
20:17:49 01-23 [epoch 6 train] [step 200/386] [eta 0:00:19]: lr 0.0003(0.0003) | loss 1.6816(1.6419)
20:17:51 01-23 [epoch 6 train] [step 220/386] [eta 0:00:17]: lr 0.0003(0.0003) | loss 1.6851(1.7205)
20:17:53 01-23 [epoch 6 train] [step 240/386] [eta 0:00:15]: lr 0.0003(0.0003) | loss 1.6951(1.8054)
20:17:55 01-23 [epoch 6 train] [step 260/386] [eta 0:00:12]: lr 0.0003(0.0003) | loss 1.6980(1.7328)
20:17:57 01-23 [epoch 6 train] [step 280/386] [eta 0:00:10]: lr 0.0003(0.0003) | loss 1.6960(1.6699)
20:17:59 01-23 [epoch 6 train] [step 300/386] [eta 0:00:08]: lr 0.0003(0.0003) | loss 1.6916(1.6302)
20:18:01 01-23 [epoch 6 train] [step 320/386] [eta 0:00:06]: lr 0.0003(0.0003) | loss 1.6952(1.7493)
20:18:04 01-23 [epoch 6 train] [step 340/386] [eta 0:00:04]: lr 0.0003(0.0003) | loss 1.6934(1.6636)
20:18:06 01-23 [epoch 6 train] [step 360/386] [eta 0:00:02]: lr 0.0003(0.0003) | loss 1.6960(1.7413)
20:18:08 01-23 [epoch 6 train] [step 380/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6903(1.5863)
20:18:08 01-23 [epoch 6 train] [step 386/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6954(1.7435)
20:18:21 01-23 [epoch 6 val]: Accuracy 59.3750 | Loss 1.6267
20:18:22 01-23 [epoch 7 train] [step 1/386] [eta 0:05:00]: lr 0.0003(0.0003) | loss 1.4293(1.7145)
20:18:24 01-23 [epoch 7 train] [step 20/386] [eta 0:04:44]: lr 0.0003(0.0003) | loss 1.7002(1.7002)
20:18:26 01-23 [epoch 7 train] [step 40/386] [eta 0:00:34]: lr 0.0003(0.0003) | loss 1.6824(1.6646)
20:18:28 01-23 [epoch 7 train] [step 60/386] [eta 0:00:34]: lr 0.0003(0.0003) | loss 1.6698(1.6446)
20:18:30 01-23 [epoch 7 train] [step 80/386] [eta 0:00:32]: lr 0.0003(0.0003) | loss 1.6712(1.6754)
20:18:32 01-23 [epoch 7 train] [step 100/386] [eta 0:00:29]: lr 0.0003(0.0003) | loss 1.6767(1.6989)
20:18:34 01-23 [epoch 7 train] [step 120/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.7003(1.8179)
20:18:36 01-23 [epoch 7 train] [step 140/386] [eta 0:00:25]: lr 0.0003(0.0003) | loss 1.6894(1.6240)
20:18:38 01-23 [epoch 7 train] [step 160/386] [eta 0:00:22]: lr 0.0003(0.0003) | loss 1.6917(1.7081)
20:18:40 01-23 [epoch 7 train] [step 180/386] [eta 0:00:20]: lr 0.0003(0.0003) | loss 1.6831(1.6141)
20:18:42 01-23 [epoch 7 train] [step 200/386] [eta 0:00:19]: lr 0.0003(0.0003) | loss 1.6717(1.5696)
20:18:44 01-23 [epoch 7 train] [step 220/386] [eta 0:00:16]: lr 0.0003(0.0003) | loss 1.6668(1.6172)
20:18:46 01-23 [epoch 7 train] [step 240/386] [eta 0:00:14]: lr 0.0003(0.0003) | loss 1.6679(1.6801)
20:18:48 01-23 [epoch 7 train] [step 260/386] [eta 0:00:12]: lr 0.0003(0.0003) | loss 1.6749(1.7584)
20:18:50 01-23 [epoch 7 train] [step 280/386] [eta 0:00:10]: lr 0.0003(0.0003) | loss 1.6729(1.6477)
20:18:53 01-23 [epoch 7 train] [step 300/386] [eta 0:00:09]: lr 0.0003(0.0003) | loss 1.6589(1.4632)
20:18:55 01-23 [epoch 7 train] [step 320/386] [eta 0:00:06]: lr 0.0003(0.0003) | loss 1.6630(1.7235)
20:18:57 01-23 [epoch 7 train] [step 340/386] [eta 0:00:04]: lr 0.0003(0.0003) | loss 1.6620(1.6458)
20:18:59 01-23 [epoch 7 train] [step 360/386] [eta 0:00:02]: lr 0.0003(0.0003) | loss 1.6639(1.6967)
20:19:01 01-23 [epoch 7 train] [step 380/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6663(1.7095)
20:19:01 01-23 [epoch 7 train] [step 386/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6668(1.5897)
20:19:14 01-23 [epoch 7 val]: Accuracy 53.1250 | Loss 1.6448
20:19:15 01-23 [epoch 8 train] [step 1/386] [eta 0:04:57]: lr 0.0003(0.0003) | loss 1.3818(1.5965)
20:19:17 01-23 [epoch 8 train] [step 20/386] [eta 0:04:45]: lr 0.0003(0.0003) | loss 1.6337(1.6337)
20:19:19 01-23 [epoch 8 train] [step 40/386] [eta 0:00:33]: lr 0.0003(0.0003) | loss 1.6683(1.7029)
20:19:21 01-23 [epoch 8 train] [step 60/386] [eta 0:00:31]: lr 0.0003(0.0003) | loss 1.6493(1.6114)
20:19:23 01-23 [epoch 8 train] [step 80/386] [eta 0:00:29]: lr 0.0003(0.0003) | loss 1.6303(1.5734)
20:19:25 01-23 [epoch 8 train] [step 100/386] [eta 0:00:29]: lr 0.0003(0.0003) | loss 1.6612(1.7844)
20:19:27 01-23 [epoch 8 train] [step 120/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.6779(1.7616)
20:19:29 01-23 [epoch 8 train] [step 140/386] [eta 0:00:24]: lr 0.0003(0.0003) | loss 1.6701(1.6232)
20:19:31 01-23 [epoch 8 train] [step 160/386] [eta 0:00:24]: lr 0.0003(0.0003) | loss 1.6595(1.5852)
20:19:33 01-23 [epoch 8 train] [step 180/386] [eta 0:00:21]: lr 0.0003(0.0003) | loss 1.6598(1.6621)
20:19:35 01-23 [epoch 8 train] [step 200/386] [eta 0:00:20]: lr 0.0003(0.0003) | loss 1.6518(1.5800)
20:19:37 01-23 [epoch 8 train] [step 220/386] [eta 0:00:17]: lr 0.0003(0.0003) | loss 1.6651(1.7985)
20:19:39 01-23 [epoch 8 train] [step 240/386] [eta 0:00:15]: lr 0.0003(0.0003) | loss 1.6690(1.7121)
20:19:41 01-23 [epoch 8 train] [step 260/386] [eta 0:00:12]: lr 0.0003(0.0003) | loss 1.6687(1.6645)
20:19:44 01-23 [epoch 8 train] [step 280/386] [eta 0:00:11]: lr 0.0003(0.0003) | loss 1.6622(1.5771)
20:19:46 01-23 [epoch 8 train] [step 300/386] [eta 0:00:08]: lr 0.0003(0.0003) | loss 1.6615(1.6521)
20:19:48 01-23 [epoch 8 train] [step 320/386] [eta 0:00:06]: lr 0.0003(0.0003) | loss 1.6591(1.6237)
20:19:50 01-23 [epoch 8 train] [step 340/386] [eta 0:00:04]: lr 0.0003(0.0003) | loss 1.6586(1.6506)
20:19:52 01-23 [epoch 8 train] [step 360/386] [eta 0:00:02]: lr 0.0003(0.0003) | loss 1.6548(1.5898)
20:19:54 01-23 [epoch 8 train] [step 380/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6560(1.6783)
20:19:55 01-23 [epoch 8 train] [step 386/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6549(1.6260)
20:20:08 01-23 [epoch 8 val]: Accuracy 58.3333 | Loss 1.6103
20:20:08 01-23 [epoch 9 train] [step 1/386] [eta 0:04:58]: lr 0.0003(0.0003) | loss 1.5175(1.6328)
20:20:10 01-23 [epoch 9 train] [step 20/386] [eta 0:04:43]: lr 0.0003(0.0003) | loss 1.5846(1.5846)
20:20:12 01-23 [epoch 9 train] [step 40/386] [eta 0:00:34]: lr 0.0003(0.0003) | loss 1.5614(1.5382)
20:20:14 01-23 [epoch 9 train] [step 60/386] [eta 0:00:33]: lr 0.0003(0.0003) | loss 1.5581(1.5514)
20:20:16 01-23 [epoch 9 train] [step 80/386] [eta 0:00:31]: lr 0.0003(0.0003) | loss 1.5670(1.5939)
20:20:18 01-23 [epoch 9 train] [step 100/386] [eta 0:00:29]: lr 0.0003(0.0003) | loss 1.5868(1.6661)
20:20:20 01-23 [epoch 9 train] [step 120/386] [eta 0:00:26]: lr 0.0003(0.0003) | loss 1.5926(1.6216)
20:20:22 01-23 [epoch 9 train] [step 140/386] [eta 0:00:25]: lr 0.0003(0.0003) | loss 1.5856(1.5437)
20:20:24 01-23 [epoch 9 train] [step 160/386] [eta 0:00:23]: lr 0.0003(0.0003) | loss 1.5944(1.6559)
20:20:27 01-23 [epoch 9 train] [step 180/386] [eta 0:00:21]: lr 0.0003(0.0003) | loss 1.6071(1.7085)
20:20:29 01-23 [epoch 9 train] [step 200/386] [eta 0:00:18]: lr 0.0003(0.0003) | loss 1.6217(1.7529)
20:20:31 01-23 [epoch 9 train] [step 220/386] [eta 0:00:16]: lr 0.0003(0.0003) | loss 1.6297(1.7103)
20:20:33 01-23 [epoch 9 train] [step 240/386] [eta 0:00:14]: lr 0.0003(0.0003) | loss 1.6326(1.6640)
20:20:35 01-23 [epoch 9 train] [step 260/386] [eta 0:00:12]: lr 0.0003(0.0003) | loss 1.6335(1.6439)
20:20:37 01-23 [epoch 9 train] [step 280/386] [eta 0:00:10]: lr 0.0003(0.0003) | loss 1.6383(1.7007)
20:20:39 01-23 [epoch 9 train] [step 300/386] [eta 0:00:08]: lr 0.0003(0.0003) | loss 1.6400(1.6644)
20:20:41 01-23 [epoch 9 train] [step 320/386] [eta 0:00:07]: lr 0.0003(0.0003) | loss 1.6340(1.5441)
20:20:43 01-23 [epoch 9 train] [step 340/386] [eta 0:00:05]: lr 0.0003(0.0003) | loss 1.6333(1.6224)
20:20:45 01-23 [epoch 9 train] [step 360/386] [eta 0:00:02]: lr 0.0003(0.0003) | loss 1.6300(1.5742)
20:20:47 01-23 [epoch 9 train] [step 380/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6240(1.5155)
20:20:48 01-23 [epoch 9 train] [step 386/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6200(1.4752)
20:21:01 01-23 [epoch 9 val]: Accuracy 61.9792 | Loss 1.5558
20:21:02 01-23 [epoch 10 train] [step 1/386] [eta 0:05:03]: lr 0.0003(0.0003) | loss 1.5725(1.4797)
20:21:04 01-23 [epoch 10 train] [step 20/386] [eta 0:04:48]: lr 0.0003(0.0003) | loss 1.6286(1.6286)
20:21:06 01-23 [epoch 10 train] [step 40/386] [eta 0:00:36]: lr 0.0003(0.0003) | loss 1.5937(1.5588)
20:21:08 01-23 [epoch 10 train] [step 60/386] [eta 0:00:35]: lr 0.0003(0.0003) | loss 1.5658(1.5100)
20:21:10 01-23 [epoch 10 train] [step 80/386] [eta 0:00:30]: lr 0.0003(0.0003) | loss 1.5945(1.6807)
20:21:12 01-23 [epoch 10 train] [step 100/386] [eta 0:00:30]: lr 0.0003(0.0003) | loss 1.6024(1.6340)
20:21:14 01-23 [epoch 10 train] [step 120/386] [eta 0:00:28]: lr 0.0003(0.0003) | loss 1.5893(1.5238)
20:21:40 01-23 [epoch 10 train] [step 140/386] [eta 0:00:25]: lr 0.0003(0.0003) | loss 1.5914(1.6043)
20:21:18 01-23 [epoch 10 train] [step 160/386] [eta 0:00:24]: lr 0.0003(0.0003) | loss 1.6005(1.6641)
20:21:21 01-23 [epoch 10 train] [step 180/386] [eta 0:00:22]: lr 0.0003(0.0003) | loss 1.6033(1.6257)
20:21:23 01-23 [epoch 10 train] [step 200/386] [eta 0:00:20]: lr 0.0003(0.0003) | loss 1.6098(1.6678)
20:21:25 01-23 [epoch 10 train] [step 220/386] [eta 0:00:17]: lr 0.0003(0.0003) | loss 1.6047(1.5543)
20:21:27 01-23 [epoch 10 train] [step 240/386] [eta 0:00:15]: lr 0.0003(0.0003) | loss 1.6041(1.5971)
20:21:29 01-23 [epoch 10 train] [step 260/386] [eta 0:00:13]: lr 0.0003(0.0003) | loss 1.6101(1.6819)
20:21:55 01-23 [epoch 10 train] [step 280/386] [eta 0:00:11]: lr 0.0003(0.0003) | loss 1.6137(1.6604)
20:21:33 01-23 [epoch 10 train] [step 300/386] [eta 0:00:08]: lr 0.0003(0.0003) | loss 1.6153(1.6376)
20:21:36 01-23 [epoch 10 train] [step 320/386] [eta 0:00:07]: lr 0.0003(0.0003) | loss 1.6121(1.5641)
20:21:38 01-23 [epoch 10 train] [step 340/386] [eta 0:00:05]: lr 0.0003(0.0003) | loss 1.6155(1.6701)
20:21:40 01-23 [epoch 10 train] [step 360/386] [eta 0:00:02]: lr 0.0003(0.0003) | loss 1.6097(1.5107)
20:21:42 01-23 [epoch 10 train] [step 380/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6086(1.5894)
20:21:43 01-23 [epoch 10 train] [step 386/386] [eta 0:00:00]: lr 0.0003(0.0003) | loss 1.6089(1.5641)
20:21:57 01-23 [epoch 10 val]: Accuracy 61.4583 | Loss 1.5169
20:21:57 01-23 [epoch 11 train] [step 1/386] [eta 0:05:09]: lr 0.0001(0.0003) | loss 1.5353(1.5798)
20:21:59 01-23 [epoch 11 train] [step 20/386] [eta 0:04:54]: lr 0.0001(0.0001) | loss 1.5469(1.5469)
20:22:01 01-23 [epoch 11 train] [step 40/386] [eta 0:00:36]: lr 0.0001(0.0001) | loss 1.6709(1.7948)
20:22:03 01-23 [epoch 11 train] [step 60/386] [eta 0:00:32]: lr 0.0001(0.0001) | loss 1.6535(1.6187)
20:22:05 01-23 [epoch 11 train] [step 80/386] [eta 0:00:31]: lr 0.0001(0.0001) | loss 1.6467(1.6264)
20:22:07 01-23 [epoch 11 train] [step 100/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.6329(1.5778)
20:22:09 01-23 [epoch 11 train] [step 120/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.6344(1.6421)
20:22:12 01-23 [epoch 11 train] [step 140/386] [eta 0:00:26]: lr 0.0001(0.0001) | loss 1.6528(1.7629)
20:22:14 01-23 [epoch 11 train] [step 160/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.6397(1.5481)
20:22:16 01-23 [epoch 11 train] [step 180/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.6345(1.5931)
20:22:18 01-23 [epoch 11 train] [step 200/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.6348(1.6377)
20:22:21 01-23 [epoch 11 train] [step 220/386] [eta 0:00:20]: lr 0.0001(0.0001) | loss 1.6200(1.4712)
20:22:23 01-23 [epoch 11 train] [step 240/386] [eta 0:00:17]: lr 0.0001(0.0001) | loss 1.6181(1.5979)
20:22:25 01-23 [epoch 11 train] [step 260/386] [eta 0:00:13]: lr 0.0001(0.0001) | loss 1.6097(1.5091)
20:22:27 01-23 [epoch 11 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.6117(1.6373)
20:22:30 01-23 [epoch 11 train] [step 300/386] [eta 0:00:09]: lr 0.0001(0.0001) | loss 1.6145(1.6537)
20:22:32 01-23 [epoch 11 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.6060(1.4785)
20:22:34 01-23 [epoch 11 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.6034(1.5621)
20:22:36 01-23 [epoch 11 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.6048(1.6283)
20:22:38 01-23 [epoch 11 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5986(1.4877)
20:22:39 01-23 [epoch 11 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.6009(1.5641)
20:22:52 01-23 [epoch 11 val]: Accuracy 59.8958 | Loss 1.5572
20:22:53 01-23 [epoch 12 train] [step 1/386] [eta 0:05:07]: lr 0.0001(0.0001) | loss 1.6396(1.5760)
20:22:55 01-23 [epoch 12 train] [step 20/386] [eta 0:04:52]: lr 0.0001(0.0001) | loss 1.5014(1.5014)
20:22:57 01-23 [epoch 12 train] [step 40/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.5026(1.5038)
20:22:59 01-23 [epoch 12 train] [step 60/386] [eta 0:00:31]: lr 0.0001(0.0001) | loss 1.4991(1.4922)
20:23:01 01-23 [epoch 12 train] [step 80/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.5405(1.6645)
20:23:26 01-23 [epoch 12 train] [step 100/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.5794(1.7353)
20:23:28 01-23 [epoch 12 train] [step 120/386] [eta 0:00:25]: lr 0.0001(0.0001) | loss 1.5812(1.5901)
20:23:30 01-23 [epoch 12 train] [step 140/386] [eta 0:00:26]: lr 0.0001(0.0001) | loss 1.5797(1.5709)
20:23:32 01-23 [epoch 12 train] [step 160/386] [eta 0:00:23]: lr 0.0001(0.0001) | loss 1.5884(1.6495)
20:23:34 01-23 [epoch 12 train] [step 180/386] [eta 0:00:20]: lr 0.0001(0.0001) | loss 1.6020(1.7105)
20:23:36 01-23 [epoch 12 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.6017(1.5992)
20:23:38 01-23 [epoch 12 train] [step 220/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5964(1.5429)
20:23:17 01-23 [epoch 12 train] [step 240/386] [eta 0:00:14]: lr 0.0001(0.0001) | loss 1.5989(1.6270)
20:23:19 01-23 [epoch 12 train] [step 260/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5952(1.5509)
20:23:45 01-23 [epoch 12 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.5894(1.5133)
20:23:23 01-23 [epoch 12 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5925(1.6365)
20:23:26 01-23 [epoch 12 train] [step 320/386] [eta 0:00:07]: lr 0.0001(0.0001) | loss 1.5907(1.5629)
20:23:28 01-23 [epoch 12 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5951(1.6654)
20:23:30 01-23 [epoch 12 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5919(1.5377)
20:23:32 01-23 [epoch 12 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5960(1.6697)
20:23:32 01-23 [epoch 12 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5951(1.6444)
20:24:09 01-23 [epoch 12 val]: Accuracy 66.1458 | Loss 1.5169
20:24:09 01-23 [epoch 13 train] [step 1/386] [eta 0:04:58]: lr 0.0001(0.0001) | loss 1.6063(1.6619)
20:24:11 01-23 [epoch 13 train] [step 20/386] [eta 0:04:43]: lr 0.0001(0.0001) | loss 1.5887(1.5887)
20:24:13 01-23 [epoch 13 train] [step 40/386] [eta 0:00:36]: lr 0.0001(0.0001) | loss 1.5346(1.4805)
20:24:15 01-23 [epoch 13 train] [step 60/386] [eta 0:00:32]: lr 0.0001(0.0001) | loss 1.5637(1.6220)
20:24:17 01-23 [epoch 13 train] [step 80/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.5763(1.6141)
20:24:19 01-23 [epoch 13 train] [step 100/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.5887(1.6381)
20:23:58 01-23 [epoch 13 train] [step 120/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.6032(1.6758)
20:24:00 01-23 [epoch 13 train] [step 140/386] [eta 0:00:25]: lr 0.0001(0.0001) | loss 1.6068(1.6283)
20:24:02 01-23 [epoch 13 train] [step 160/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.5949(1.5115)
20:24:04 01-23 [epoch 13 train] [step 180/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5850(1.5058)
20:24:30 01-23 [epoch 13 train] [step 200/386] [eta 0:00:20]: lr 0.0001(0.0001) | loss 1.5892(1.6278)
20:24:09 01-23 [epoch 13 train] [step 220/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5839(1.5306)
20:24:11 01-23 [epoch 13 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5811(1.5506)
20:24:13 01-23 [epoch 13 train] [step 260/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5918(1.7201)
20:24:15 01-23 [epoch 13 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.6019(1.7326)
20:24:17 01-23 [epoch 13 train] [step 300/386] [eta 0:00:09]: lr 0.0001(0.0001) | loss 1.5989(1.5565)
20:24:19 01-23 [epoch 13 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5915(1.4815)
20:24:21 01-23 [epoch 13 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5929(1.6144)
20:24:23 01-23 [epoch 13 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5925(1.5858)
20:24:25 01-23 [epoch 13 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5921(1.5848)
20:24:26 01-23 [epoch 13 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5904(1.5407)
20:25:02 01-23 [epoch 13 val]: Accuracy 64.0625 | Loss 1.5098
20:25:03 01-23 [epoch 14 train] [step 1/386] [eta 0:04:57]: lr 0.0001(0.0001) | loss 1.5861(1.5413)
20:25:05 01-23 [epoch 14 train] [step 20/386] [eta 0:04:46]: lr 0.0001(0.0001) | loss 1.7240(1.7240)
20:25:07 01-23 [epoch 14 train] [step 40/386] [eta 0:00:34]: lr 0.0001(0.0001) | loss 1.6023(1.4805)
20:25:09 01-23 [epoch 14 train] [step 60/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.6152(1.6411)
20:24:48 01-23 [epoch 14 train] [step 80/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.6241(1.6509)
20:24:50 01-23 [epoch 14 train] [step 100/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.6061(1.5338)
20:24:52 01-23 [epoch 14 train] [step 120/386] [eta 0:00:27]: lr 0.0001(0.0001) | loss 1.6140(1.6536)
20:24:54 01-23 [epoch 14 train] [step 140/386] [eta 0:00:26]: lr 0.0001(0.0001) | loss 1.6212(1.6645)
20:24:56 01-23 [epoch 14 train] [step 160/386] [eta 0:00:23]: lr 0.0001(0.0001) | loss 1.5943(1.4058)
20:24:58 01-23 [epoch 14 train] [step 180/386] [eta 0:00:21]: lr 0.0001(0.0001) | loss 1.5978(1.6258)
20:25:01 01-23 [epoch 14 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5891(1.5110)
20:25:03 01-23 [epoch 14 train] [step 220/386] [eta 0:00:17]: lr 0.0001(0.0001) | loss 1.5879(1.5754)
20:25:05 01-23 [epoch 14 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5799(1.4918)
20:25:07 01-23 [epoch 14 train] [step 260/386] [eta 0:00:13]: lr 0.0001(0.0001) | loss 1.5841(1.6351)
20:25:09 01-23 [epoch 14 train] [step 280/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5816(1.5489)
20:25:11 01-23 [epoch 14 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5748(1.4802)
20:25:13 01-23 [epoch 14 train] [step 320/386] [eta 0:00:07]: lr 0.0001(0.0001) | loss 1.5757(1.5880)
20:25:16 01-23 [epoch 14 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5694(1.4701)
20:25:18 01-23 [epoch 14 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5822(1.7996)
20:25:20 01-23 [epoch 14 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5767(1.4780)
20:25:20 01-23 [epoch 14 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5764(1.5165)
20:25:34 01-23 [epoch 14 val]: Accuracy 61.4583 | Loss 1.5318
20:25:34 01-23 [epoch 15 train] [step 1/386] [eta 0:05:10]: lr 0.0001(0.0001) | loss 1.1448(1.5171)
20:25:36 01-23 [epoch 15 train] [step 20/386] [eta 0:04:55]: lr 0.0001(0.0001) | loss 1.6645(1.6645)
20:25:39 01-23 [epoch 15 train] [step 40/386] [eta 0:00:36]: lr 0.0001(0.0001) | loss 1.6258(1.5870)
20:25:41 01-23 [epoch 15 train] [step 60/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.5920(1.5244)
20:25:43 01-23 [epoch 15 train] [step 80/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.5850(1.5641)
20:25:45 01-23 [epoch 15 train] [step 100/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.5802(1.5611)
20:25:47 01-23 [epoch 15 train] [step 120/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.5576(1.4446)
20:25:49 01-23 [epoch 15 train] [step 140/386] [eta 0:00:26]: lr 0.0001(0.0001) | loss 1.5541(1.5331)
20:26:15 01-23 [epoch 15 train] [step 160/386] [eta 0:00:25]: lr 0.0001(0.0001) | loss 1.5644(1.6367)
20:25:54 01-23 [epoch 15 train] [step 180/386] [eta 0:00:21]: lr 0.0001(0.0001) | loss 1.5499(1.4335)
20:25:56 01-23 [epoch 15 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5622(1.6724)
20:25:58 01-23 [epoch 15 train] [step 220/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5578(1.5146)
20:26:00 01-23 [epoch 15 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5499(1.4622)
20:26:02 01-23 [epoch 15 train] [step 260/386] [eta 0:00:13]: lr 0.0001(0.0001) | loss 1.5598(1.6796)
20:26:04 01-23 [epoch 15 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.5607(1.5724)
20:26:30 01-23 [epoch 15 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5690(1.6847)
20:26:09 01-23 [epoch 15 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5622(1.4606)
20:26:11 01-23 [epoch 15 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5667(1.6387)
20:26:13 01-23 [epoch 15 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5678(1.5866)
20:26:15 01-23 [epoch 15 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5704(1.6170)
20:26:15 01-23 [epoch 15 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5688(1.5314)
20:26:29 01-23 [epoch 15 val]: Accuracy 61.4583 | Loss 1.5114
20:26:30 01-23 [epoch 16 train] [step 1/386] [eta 0:05:09]: lr 0.0001(0.0001) | loss 1.2392(1.5330)
20:26:55 01-23 [epoch 16 train] [step 20/386] [eta 0:04:57]: lr 0.0001(0.0001) | loss 1.5123(1.5123)
20:26:34 01-23 [epoch 16 train] [step 40/386] [eta 0:00:37]: lr 0.0001(0.0001) | loss 1.5252(1.5381)
20:26:36 01-23 [epoch 16 train] [step 60/386] [eta 0:00:34]: lr 0.0001(0.0001) | loss 1.5225(1.5171)
20:26:38 01-23 [epoch 16 train] [step 80/386] [eta 0:00:32]: lr 0.0001(0.0001) | loss 1.5219(1.5199)
20:26:40 01-23 [epoch 16 train] [step 100/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.5486(1.6554)
20:26:42 01-23 [epoch 16 train] [step 120/386] [eta 0:00:28]: lr 0.0001(0.0001) | loss 1.5413(1.5050)
20:26:44 01-23 [epoch 16 train] [step 140/386] [eta 0:00:24]: lr 0.0001(0.0001) | loss 1.5401(1.5324)
20:26:46 01-23 [epoch 16 train] [step 160/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.5746(1.8165)
20:26:48 01-23 [epoch 16 train] [step 180/386] [eta 0:00:21]: lr 0.0001(0.0001) | loss 1.5748(1.5766)
20:26:50 01-23 [epoch 16 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5753(1.5797)
20:26:53 01-23 [epoch 16 train] [step 220/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5625(1.4348)
20:26:55 01-23 [epoch 16 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5723(1.6796)
20:26:57 01-23 [epoch 16 train] [step 260/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5741(1.5953)
20:26:59 01-23 [epoch 16 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.5830(1.6989)
20:27:01 01-23 [epoch 16 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5813(1.5571)
20:27:03 01-23 [epoch 16 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5732(1.4523)
20:27:05 01-23 [epoch 16 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5688(1.4989)
20:27:07 01-23 [epoch 16 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5698(1.5861)
20:27:09 01-23 [epoch 16 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5756(1.6796)
20:27:10 01-23 [epoch 16 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5735(1.6439)
20:27:47 01-23 [epoch 16 val]: Accuracy 61.4583 | Loss 1.4852
20:27:47 01-23 [epoch 17 train] [step 1/386] [eta 0:05:05]: lr 0.0001(0.0001) | loss 1.8522(1.6181)
20:27:49 01-23 [epoch 17 train] [step 20/386] [eta 0:04:51]: lr 0.0001(0.0001) | loss 1.5640(1.5640)
20:27:51 01-23 [epoch 17 train] [step 40/386] [eta 0:00:38]: lr 0.0001(0.0001) | loss 1.5648(1.5657)
20:27:53 01-23 [epoch 17 train] [step 60/386] [eta 0:00:32]: lr 0.0001(0.0001) | loss 1.5485(1.5159)
20:27:32 01-23 [epoch 17 train] [step 80/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.5576(1.5849)
20:27:34 01-23 [epoch 17 train] [step 100/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.5398(1.4688)
20:27:36 01-23 [epoch 17 train] [step 120/386] [eta 0:00:27]: lr 0.0001(0.0001) | loss 1.5503(1.6028)
20:27:39 01-23 [epoch 17 train] [step 140/386] [eta 0:00:30]: lr 0.0001(0.0001) | loss 1.5424(1.4948)
20:27:41 01-23 [epoch 17 train] [step 160/386] [eta 0:00:28]: lr 0.0001(0.0001) | loss 1.5607(1.6884)
20:27:43 01-23 [epoch 17 train] [step 180/386] [eta 0:00:21]: lr 0.0001(0.0001) | loss 1.5547(1.5072)
20:27:45 01-23 [epoch 17 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5612(1.6195)
20:27:47 01-23 [epoch 17 train] [step 220/386] [eta 0:00:17]: lr 0.0001(0.0001) | loss 1.5706(1.6643)
20:27:50 01-23 [epoch 17 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5769(1.6467)
20:28:15 01-23 [epoch 17 train] [step 260/386] [eta 0:00:13]: lr 0.0001(0.0001) | loss 1.5887(1.7307)
20:27:54 01-23 [epoch 17 train] [step 280/386] [eta 0:00:10]: lr 0.0001(0.0001) | loss 1.5887(1.5875)
20:27:56 01-23 [epoch 17 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5824(1.4948)
20:27:58 01-23 [epoch 17 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5816(1.5702)
20:28:00 01-23 [epoch 17 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5779(1.5172)
20:28:02 01-23 [epoch 17 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5702(1.4405)
20:28:04 01-23 [epoch 17 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5655(1.4812)
20:28:05 01-23 [epoch 17 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5632(1.4910)
20:28:18 01-23 [epoch 17 val]: Accuracy 63.5417 | Loss 1.4890
20:28:19 01-23 [epoch 18 train] [step 1/386] [eta 0:05:02]: lr 0.0001(0.0001) | loss 1.2071(1.4751)
20:28:21 01-23 [epoch 18 train] [step 20/386] [eta 0:04:50]: lr 0.0001(0.0001) | loss 1.6177(1.6177)
20:28:23 01-23 [epoch 18 train] [step 40/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.5733(1.5288)
20:28:25 01-23 [epoch 18 train] [step 60/386] [eta 0:00:33]: lr 0.0001(0.0001) | loss 1.5679(1.5571)
20:28:27 01-23 [epoch 18 train] [step 80/386] [eta 0:00:31]: lr 0.0001(0.0001) | loss 1.5777(1.6073)
20:28:29 01-23 [epoch 18 train] [step 100/386] [eta 0:00:28]: lr 0.0001(0.0001) | loss 1.5762(1.5701)
20:28:31 01-23 [epoch 18 train] [step 120/386] [eta 0:00:27]: lr 0.0001(0.0001) | loss 1.5578(1.4657)
20:28:33 01-23 [epoch 18 train] [step 140/386] [eta 0:00:23]: lr 0.0001(0.0001) | loss 1.5464(1.4778)
20:28:35 01-23 [epoch 18 train] [step 160/386] [eta 0:00:22]: lr 0.0001(0.0001) | loss 1.5455(1.5395)
20:28:37 01-23 [epoch 18 train] [step 180/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5489(1.5764)
20:28:39 01-23 [epoch 18 train] [step 200/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5519(1.5789)
20:28:40 01-23 [epoch 18 train] [step 220/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5480(1.5089)
20:28:42 01-23 [epoch 18 train] [step 240/386] [eta 0:00:14]: lr 0.0001(0.0001) | loss 1.5528(1.6058)
20:28:44 01-23 [epoch 18 train] [step 260/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5526(1.5492)
20:28:46 01-23 [epoch 18 train] [step 280/386] [eta 0:00:11]: lr 0.0001(0.0001) | loss 1.5656(1.7357)
20:28:48 01-23 [epoch 18 train] [step 300/386] [eta 0:00:08]: lr 0.0001(0.0001) | loss 1.5711(1.6472)
20:28:50 01-23 [epoch 18 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5649(1.4729)
20:28:52 01-23 [epoch 18 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5587(1.4581)
20:28:54 01-23 [epoch 18 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5516(1.4318)
20:28:57 01-23 [epoch 18 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5519(1.5570)
20:28:57 01-23 [epoch 18 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5506(1.5099)
20:29:11 01-23 [epoch 18 val]: Accuracy 64.0625 | Loss 1.4708
20:29:11 01-23 [epoch 19 train] [step 1/386] [eta 0:05:08]: lr 0.0001(0.0001) | loss 1.7066(1.4912)
20:29:13 01-23 [epoch 19 train] [step 20/386] [eta 0:04:54]: lr 0.0001(0.0001) | loss 1.5062(1.5062)
20:29:15 01-23 [epoch 19 train] [step 40/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.5292(1.5522)
20:29:17 01-23 [epoch 19 train] [step 60/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.4981(1.4357)
20:29:20 01-23 [epoch 19 train] [step 80/386] [eta 0:00:31]: lr 0.0001(0.0001) | loss 1.5197(1.5845)
20:29:22 01-23 [epoch 19 train] [step 100/386] [eta 0:00:29]: lr 0.0001(0.0001) | loss 1.5484(1.6633)
20:29:24 01-23 [epoch 19 train] [step 120/386] [eta 0:00:27]: lr 0.0001(0.0001) | loss 1.5438(1.5208)
20:29:26 01-23 [epoch 19 train] [step 140/386] [eta 0:00:26]: lr 0.0001(0.0001) | loss 1.5479(1.5723)
20:29:28 01-23 [epoch 19 train] [step 160/386] [eta 0:00:25]: lr 0.0001(0.0001) | loss 1.5446(1.5216)
20:29:30 01-23 [epoch 19 train] [step 180/386] [eta 0:00:21]: lr 0.0001(0.0001) | loss 1.5636(1.7156)
20:29:32 01-23 [epoch 19 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5776(1.7034)
20:29:34 01-23 [epoch 19 train] [step 220/386] [eta 0:00:18]: lr 0.0001(0.0001) | loss 1.5746(1.5447)
20:29:37 01-23 [epoch 19 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5804(1.6440)
20:29:39 01-23 [epoch 19 train] [step 260/386] [eta 0:00:12]: lr 0.0001(0.0001) | loss 1.5770(1.5366)
20:29:41 01-23 [epoch 19 train] [step 280/386] [eta 0:00:10]: lr 0.0001(0.0001) | loss 1.5758(1.5608)
20:29:43 01-23 [epoch 19 train] [step 300/386] [eta 0:00:09]: lr 0.0001(0.0001) | loss 1.5806(1.6477)
20:29:45 01-23 [epoch 19 train] [step 320/386] [eta 0:00:07]: lr 0.0001(0.0001) | loss 1.5782(1.5418)
20:29:47 01-23 [epoch 19 train] [step 340/386] [eta 0:00:05]: lr 0.0001(0.0001) | loss 1.5749(1.5214)
20:29:49 01-23 [epoch 19 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5780(1.6309)
20:29:51 01-23 [epoch 19 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5752(1.5253)
20:29:52 01-23 [epoch 19 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5779(1.6172)
20:30:06 01-23 [epoch 19 val]: Accuracy 60.9375 | Loss 1.4937
20:30:06 01-23 [epoch 20 train] [step 1/386] [eta 0:05:09]: lr 0.0001(0.0001) | loss 1.1072(1.5680)
20:30:08 01-23 [epoch 20 train] [step 20/386] [eta 0:04:56]: lr 0.0001(0.0001) | loss 1.4435(1.4435)
20:30:10 01-23 [epoch 20 train] [step 40/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.4957(1.5479)
20:30:12 01-23 [epoch 20 train] [step 60/386] [eta 0:00:35]: lr 0.0001(0.0001) | loss 1.4965(1.4981)
20:30:15 01-23 [epoch 20 train] [step 80/386] [eta 0:00:32]: lr 0.0001(0.0001) | loss 1.5015(1.5163)
20:30:17 01-23 [epoch 20 train] [step 100/386] [eta 0:00:28]: lr 0.0001(0.0001) | loss 1.4802(1.3952)
20:30:19 01-23 [epoch 20 train] [step 120/386] [eta 0:00:27]: lr 0.0001(0.0001) | loss 1.4965(1.5777)
20:30:21 01-23 [epoch 20 train] [step 140/386] [eta 0:00:24]: lr 0.0001(0.0001) | loss 1.5179(1.6464)
20:30:23 01-23 [epoch 20 train] [step 160/386] [eta 0:00:23]: lr 0.0001(0.0001) | loss 1.5130(1.4787)
20:30:25 01-23 [epoch 20 train] [step 180/386] [eta 0:00:20]: lr 0.0001(0.0001) | loss 1.5294(1.6604)
20:30:27 01-23 [epoch 20 train] [step 200/386] [eta 0:00:19]: lr 0.0001(0.0001) | loss 1.5270(1.5061)
20:30:29 01-23 [epoch 20 train] [step 220/386] [eta 0:00:16]: lr 0.0001(0.0001) | loss 1.5301(1.5608)
20:30:31 01-23 [epoch 20 train] [step 240/386] [eta 0:00:15]: lr 0.0001(0.0001) | loss 1.5300(1.5291)
20:30:33 01-23 [epoch 20 train] [step 260/386] [eta 0:00:13]: lr 0.0001(0.0001) | loss 1.5203(1.4039)
20:30:35 01-23 [epoch 20 train] [step 280/386] [eta 0:00:10]: lr 0.0001(0.0001) | loss 1.5287(1.6374)
20:30:37 01-23 [epoch 20 train] [step 300/386] [eta 0:00:09]: lr 0.0001(0.0001) | loss 1.5321(1.5803)
20:30:39 01-23 [epoch 20 train] [step 320/386] [eta 0:00:06]: lr 0.0001(0.0001) | loss 1.5282(1.4689)
20:30:41 01-23 [epoch 20 train] [step 340/386] [eta 0:00:04]: lr 0.0001(0.0001) | loss 1.5254(1.4808)
20:30:43 01-23 [epoch 20 train] [step 360/386] [eta 0:00:02]: lr 0.0001(0.0001) | loss 1.5364(1.7235)
20:30:45 01-23 [epoch 20 train] [step 380/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5476(1.7501)
20:30:46 01-23 [epoch 20 train] [step 386/386] [eta 0:00:00]: lr 0.0001(0.0001) | loss 1.5487(1.7233)
20:31:00 01-23 [epoch 20 val]: Accuracy 61.9792 | Loss 1.4690
