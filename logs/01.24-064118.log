Log file created at 06:41:20 01-24 with path: logs/01.24-06:41:18.log
System information:
+-----------------------+------------------------------------+
|    CPU Physical Cores |                                 16 |
+-----------------------+------------------------------------+
|     CPU Logical Cores |                                 32 |
+-----------------------+------------------------------------+
|     CPU Max Frequency |                           0.00 MHz |
+-----------------------+------------------------------------+
| CPU Current Frequency |                        2419.20 MHz |
+-----------------------+------------------------------------+
|             RAM Total |                           10.70 GB |
+-----------------------+------------------------------------+
|         RAM Available |                            7.62 GB |
+-----------------------+------------------------------------+
|              RAM Used |                            2.78 GB |
+-----------------------+------------------------------------+
|            GPU Device | NVIDIA GeForce RTX 4060 Laptop GPU |
+-----------------------+------------------------------------+
|             GPU Count |                                  1 |
+-----------------------+------------------------------------+
|      GPU Memory Total |                            8.00 GB |
+-----------------------+------------------------------------+
|   GPU Memory Reserved |                            0.00 GB |
+-----------------------+------------------------------------+
|          CUDA Version |                               12.1 |
+-----------------------+------------------------------------+
|              Hostname |                            AmadFat |
+-----------------------+------------------------------------+
|                    OS |                              Linux |
+-----------------------+------------------------------------+
|            OS Release | 5.15.167.4-microsoft-standard-WSL2 |
+-----------------------+------------------------------------+
|            OS Version | #1 SMP Tue Nov 5 00:21:55 UTC 2024 |
+-----------------------+------------------------------------+
|        Python Version |                             3.9.21 |
+-----------------------+------------------------------------+
|       PyTorch Version |                        2.4.1+cu121 |
+-----------------------+------------------------------------+
|   TorchVision Version |                       0.19.1+cu121 |
+-----------------------+------------------------------------+
|    TorchAudio Version |                        2.4.1+cu121 |
+-----------------------+------------------------------------+
Experiment settings:
{'criterion': {'alpha': 5.0, 'criterion_name': 'focalloss', 'gamma': 1.5},
 'data': {'batch_size': 4,
          'num_workers': 4,
          'root': 'dataset',
          'split_ratio': [0.8, 0.2]},
 'evaluator': {'acc': True, 'loss': True},
 'experiment': {'ckpt_save_dir': PosixPath('ckpts/01.24-06:41:18'),
                'deterministic': False,
                'device': 'cuda',
                'experiment_name': '01.24-06:41:18',
                'max_epochs': 75,
                'save': True,
                'seed': 3407,
                'val': True,
                'val_interval': 1},
 'logger': {'log_save_path': PosixPath('logs/01.24-06:41:18.log'),
            'tb_save_path': PosixPath('tbevents/01.24-06:41:18'),
            'train_print_interval': 50,
            'use_tensorboard': True,
            'window_metric': 50,
            'window_time_stamp': 50},
 'model': {'dropout': 0.0,
           'last_fc_bias_init': 'const',
           'last_fc_weight_init': 'const',
           'model_name': 'mobilenet_v3_s',
           'norm_layer': 'batchnorm',
           'weights': 'default'},
 'optimizer': {'grad_clip': 1.0,
               'lr': 0.001,
               'momentum': 0.95,
               'optimizer_name': 'sgd',
               'weight_decay': 0.0001},
 'scheduler': {'T_0': 5,
               'T_mult': 2,
               'eta_min': 3e-05,
               'scheduler_name': 'cosinelr'},
 'transform': {'train': {'compose': {'transforms': [{'autoaugment': {'interpolation': 'nearest',
                                                                     'policy': 'imagenet'}},
                                                    {'totensor': {}},
                                                    {'randomflip': {'prob_hflip': 0.5,
                                                                    'prob_vflip': 0.5}},
                                                    {'colorjitter': {'contrast': 0.3}}]}},
               'val': {'totensor': {}}}}
06:42:10 01-24 [epoch 1 train] [step 1/193] [eta N/A]: loss 1.1356(1.1356) | lr 1.0000e-03
06:42:12 01-24 [epoch 1 train] [step 50/193] [eta 0:00:06]: loss 1.1064(1.1064) | lr 1.0000e-03
06:42:14 01-24 [epoch 1 train] [step 100/193] [eta 0:00:02]: loss 1.1028(1.0993) | lr 1.0000e-03
06:42:15 01-24 [epoch 1 train] [step 150/193] [eta 0:00:01]: loss 1.0767(1.0243) | lr 1.0000e-03
06:42:17 01-24 [epoch 1 train] [step 193/193] [eta 0:00:00]: loss 1.0594(0.9921) | lr 1.0000e-03
06:42:21 01-24 [epoch 1 val]: Accuracy 34.3750 | Loss 0.9885
06:42:21 01-24 [epoch 2 train] [step 1/193] [eta 0:00:23]: loss 0.9378(0.9917) | lr 9.9815e-04
06:42:23 01-24 [epoch 2 train] [step 50/193] [eta 0:00:16]: loss 0.9809(0.9809) | lr 9.0737e-04
06:42:24 01-24 [epoch 2 train] [step 100/193] [eta 0:00:02]: loss 0.9538(0.9268) | lr 9.0737e-04
06:42:26 01-24 [epoch 2 train] [step 150/193] [eta 0:00:01]: loss 0.9375(0.9048) | lr 9.0737e-04
06:42:28 01-24 [epoch 2 train] [step 193/193] [eta 0:00:00]: loss 0.9128(0.8355) | lr 9.0737e-04
06:42:32 01-24 [epoch 2 val]: Accuracy 57.8125 | Loss 0.8308
06:42:33 01-24 [epoch 3 train] [step 1/193] [eta 0:00:26]: loss 0.6153(0.8310) | lr 9.0252e-04
06:42:34 01-24 [epoch 3 train] [step 50/193] [eta 0:00:18]: loss 0.7987(0.7987) | lr 6.6487e-04
06:42:36 01-24 [epoch 3 train] [step 100/193] [eta 0:00:02]: loss 0.7952(0.7916) | lr 6.6487e-04
06:42:37 01-24 [epoch 3 train] [step 150/193] [eta 0:00:01]: loss 0.7882(0.7743) | lr 6.6487e-04
06:42:40 01-24 [epoch 3 train] [step 193/193] [eta 0:00:00]: loss 0.7815(0.7620) | lr 6.6487e-04
06:42:44 01-24 [epoch 3 val]: Accuracy 63.0208 | Loss 0.7213
06:42:45 01-24 [epoch 4 train] [step 1/193] [eta 0:00:30]: loss 0.8044(0.7640) | lr 6.5888e-04
06:42:47 01-24 [epoch 4 train] [step 50/193] [eta 0:00:19]: loss 0.7486(0.7486) | lr 3.6513e-04
06:42:49 01-24 [epoch 4 train] [step 100/193] [eta 0:00:04]: loss 0.7348(0.7210) | lr 3.6513e-04
06:42:51 01-24 [epoch 4 train] [step 150/193] [eta 0:00:01]: loss 0.7266(0.7101) | lr 3.6513e-04
06:42:53 01-24 [epoch 4 train] [step 193/193] [eta 0:00:00]: loss 0.7045(0.6427) | lr 3.6513e-04
06:42:56 01-24 [epoch 4 val]: Accuracy 63.0208 | Loss 0.6732
06:42:57 01-24 [epoch 5 train] [step 1/193] [eta 0:00:21]: loss 0.3725(0.6385) | lr 3.6028e-04
06:42:58 01-24 [epoch 5 train] [step 50/193] [eta 0:00:15]: loss 0.6905(0.6905) | lr 1.2263e-04
06:43:00 01-24 [epoch 5 train] [step 100/193] [eta 0:00:03]: loss 0.6839(0.6772) | lr 1.2263e-04
06:43:02 01-24 [epoch 5 train] [step 150/193] [eta 0:00:01]: loss 0.6877(0.6953) | lr 1.2263e-04
06:43:03 01-24 [epoch 5 train] [step 193/193] [eta 0:00:00]: loss 0.6801(0.6572) | lr 1.2263e-04
06:43:06 01-24 [epoch 5 val]: Accuracy 64.5833 | Loss 0.6659
06:43:07 01-24 [epoch 6 train] [step 1/193] [eta 0:00:21]: loss 0.8755(0.6644) | lr 1.4017e-04
06:43:08 01-24 [epoch 6 train] [step 50/193] [eta 0:00:15]: loss 0.6697(0.6697) | lr 1.0000e-03
06:43:10 01-24 [epoch 6 train] [step 100/193] [eta 0:00:02]: loss 0.6634(0.6571) | lr 1.0000e-03
06:43:12 01-24 [epoch 6 train] [step 150/193] [eta 0:00:02]: loss 0.6491(0.6206) | lr 1.0000e-03
06:43:14 01-24 [epoch 6 train] [step 193/193] [eta 0:00:00]: loss 0.6430(0.6147) | lr 1.0000e-03
06:43:17 01-24 [epoch 6 val]: Accuracy 65.1042 | Loss 0.5940
06:43:18 01-24 [epoch 7 train] [step 1/193] [eta 0:00:22]: loss 0.3785(0.6089) | lr 9.9953e-04
06:43:42 01-24 [epoch 7 train] [step 50/193] [eta 0:00:15]: loss 0.5896(0.5896) | lr 9.7626e-04
06:43:21 01-24 [epoch 7 train] [step 100/193] [eta 0:00:02]: loss 0.5731(0.5566) | lr 9.7626e-04
06:43:22 01-24 [epoch 7 train] [step 150/193] [eta 0:00:01]: loss 0.5852(0.6094) | lr 9.7626e-04
06:43:24 01-24 [epoch 7 train] [step 193/193] [eta 0:00:00]: loss 0.5760(0.5557) | lr 9.7626e-04
06:43:27 01-24 [epoch 7 val]: Accuracy 66.6667 | Loss 0.5460
06:43:28 01-24 [epoch 8 train] [step 1/193] [eta 0:00:21]: loss 1.0530(0.5662) | lr 9.7488e-04
06:43:29 01-24 [epoch 8 train] [step 50/193] [eta 0:00:16]: loss 0.5600(0.5600) | lr 9.0737e-04
06:43:31 01-24 [epoch 8 train] [step 100/193] [eta 0:00:02]: loss 0.5751(0.5902) | lr 9.0737e-04
06:43:33 01-24 [epoch 8 train] [step 150/193] [eta 0:00:01]: loss 0.5422(0.4764) | lr 9.0737e-04
06:43:58 01-24 [epoch 8 train] [step 193/193] [eta 0:00:00]: loss 0.5351(0.4955) | lr 9.0737e-04
06:43:38 01-24 [epoch 8 val]: Accuracy 68.2292 | Loss 0.5426
06:43:38 01-24 [epoch 9 train] [step 1/193] [eta 0:00:22]: loss 0.1935(0.4919) | lr 9.0523e-04
06:43:40 01-24 [epoch 9 train] [step 50/193] [eta 0:00:15]: loss 0.4846(0.4846) | lr 8.0008e-04
06:43:42 01-24 [epoch 9 train] [step 100/193] [eta 0:00:03]: loss 0.5086(0.5325) | lr 8.0008e-04
06:43:43 01-24 [epoch 9 train] [step 150/193] [eta 0:00:01]: loss 0.5030(0.4918) | lr 8.0008e-04
06:44:07 01-24 [epoch 9 train] [step 193/193] [eta 0:00:00]: loss 0.5081(0.5241) | lr 8.0008e-04
06:43:48 01-24 [epoch 9 val]: Accuracy 69.7917 | Loss 0.5004
06:43:49 01-24 [epoch 10 train] [step 1/193] [eta 0:00:24]: loss 0.6800(0.5255) | lr 7.9737e-04
06:43:52 01-24 [epoch 10 train] [step 50/193] [eta 0:00:20]: loss 0.5094(0.5094) | lr 6.6487e-04
06:43:53 01-24 [epoch 10 train] [step 100/193] [eta 0:00:02]: loss 0.5299(0.5504) | lr 6.6487e-04
06:44:18 01-24 [epoch 10 train] [step 150/193] [eta 0:00:01]: loss 0.5221(0.5064) | lr 6.6487e-04
06:44:19 01-24 [epoch 10 train] [step 193/193] [eta 0:00:00]: loss 0.5099(0.4894) | lr 6.6487e-04
06:44:23 01-24 [epoch 10 val]: Accuracy 69.7917 | Loss 0.5175
06:44:25 01-24 [epoch 11 train] [step 1/193] [eta 0:00:28]: loss 0.7241(0.4790) | lr 6.6188e-04
06:44:27 01-24 [epoch 11 train] [step 50/193] [eta 0:00:22]: loss 0.5062(0.5062) | lr 5.1500e-04
06:44:29 01-24 [epoch 11 train] [step 100/193] [eta 0:00:03]: loss 0.4915(0.4768) | lr 5.1500e-04
06:44:31 01-24 [epoch 11 train] [step 150/193] [eta 0:00:01]: loss 0.4949(0.5016) | lr 5.1500e-04
06:44:33 01-24 [epoch 11 train] [step 193/193] [eta 0:00:00]: loss 0.4963(0.5035) | lr 5.1500e-04
06:44:13 01-24 [epoch 11 val]: Accuracy 68.7500 | Loss 0.5302
06:44:14 01-24 [epoch 12 train] [step 1/193] [eta 0:00:22]: loss 0.7049(0.5017) | lr 5.1200e-04
06:44:15 01-24 [epoch 12 train] [step 50/193] [eta 0:00:16]: loss 0.4437(0.4437) | lr 3.6513e-04
06:44:17 01-24 [epoch 12 train] [step 100/193] [eta 0:00:03]: loss 0.4542(0.4647) | lr 3.6513e-04
06:44:19 01-24 [epoch 12 train] [step 150/193] [eta 0:00:01]: loss 0.4563(0.4605) | lr 3.6513e-04
06:44:21 01-24 [epoch 12 train] [step 193/193] [eta 0:00:00]: loss 0.4607(0.4551) | lr 3.6513e-04
06:44:48 01-24 [epoch 12 val]: Accuracy 69.2708 | Loss 0.5513
06:44:25 01-24 [epoch 13 train] [step 1/193] [eta 0:00:23]: loss 0.5792(0.4599) | lr 3.6242e-04
06:44:27 01-24 [epoch 13 train] [step 50/193] [eta 0:00:17]: loss 0.4514(0.4514) | lr 2.2992e-04
06:44:28 01-24 [epoch 13 train] [step 100/193] [eta 0:00:03]: loss 0.4627(0.4740) | lr 2.2992e-04
06:44:30 01-24 [epoch 13 train] [step 150/193] [eta 0:00:01]: loss 0.4652(0.4702) | lr 2.2992e-04
06:44:31 01-24 [epoch 13 train] [step 193/193] [eta 0:00:00]: loss 0.4656(0.4654) | lr 2.2992e-04
06:44:35 01-24 [epoch 13 val]: Accuracy 69.7917 | Loss 0.5257
06:44:38 01-24 [epoch 14 train] [step 1/193] [eta 0:00:30]: loss 0.7573(0.4683) | lr 2.2778e-04
06:44:40 01-24 [epoch 14 train] [step 50/193] [eta 0:00:24]: loss 0.4515(0.4515) | lr 1.2263e-04
06:44:42 01-24 [epoch 14 train] [step 100/193] [eta 0:00:04]: loss 0.4441(0.4366) | lr 1.2263e-04
06:44:44 01-24 [epoch 14 train] [step 150/193] [eta 0:00:01]: loss 0.4347(0.4159) | lr 1.2263e-04
06:44:45 01-24 [epoch 14 train] [step 193/193] [eta 0:00:00]: loss 0.4466(0.4905) | lr 1.2263e-04
06:44:49 01-24 [epoch 14 val]: Accuracy 68.2292 | Loss 0.5117
06:45:13 01-24 [epoch 15 train] [step 1/193] [eta 0:00:22]: loss 0.2990(0.4872) | lr 1.2125e-04
06:44:51 01-24 [epoch 15 train] [step 50/193] [eta 0:00:16]: loss 0.4440(0.4440) | lr 5.3738e-05
06:44:53 01-24 [epoch 15 train] [step 100/193] [eta 0:00:03]: loss 0.4482(0.4524) | lr 5.3738e-05
06:44:54 01-24 [epoch 15 train] [step 150/193] [eta 0:00:01]: loss 0.4493(0.4515) | lr 5.3738e-05
06:44:56 01-24 [epoch 15 train] [step 193/193] [eta 0:00:00]: loss 0.4458(0.4387) | lr 5.3738e-05
06:44:59 01-24 [epoch 15 val]: Accuracy 69.7917 | Loss 0.4935
06:45:00 01-24 [epoch 16 train] [step 1/193] [eta 0:00:22]: loss 0.7434(0.4425) | lr 7.2663e-05
06:45:01 01-24 [epoch 16 train] [step 50/193] [eta 0:00:16]: loss 0.4533(0.4533) | lr 1.0000e-03
06:45:03 01-24 [epoch 16 train] [step 100/193] [eta 0:00:02]: loss 0.4572(0.4611) | lr 1.0000e-03
06:45:05 01-24 [epoch 16 train] [step 150/193] [eta 0:00:01]: loss 0.4682(0.4903) | lr 1.0000e-03
06:45:06 01-24 [epoch 16 train] [step 193/193] [eta 0:00:00]: loss 0.4611(0.4476) | lr 1.0000e-03
06:45:10 01-24 [epoch 16 val]: Accuracy 68.2292 | Loss 0.5153
06:45:11 01-24 [epoch 17 train] [step 1/193] [eta 0:00:25]: loss 0.1968(0.4306) | lr 9.9988e-04
06:45:13 01-24 [epoch 17 train] [step 50/193] [eta 0:00:18]: loss 0.4272(0.4272) | lr 9.9403e-04
06:45:15 01-24 [epoch 17 train] [step 100/193] [eta 0:00:04]: loss 0.4249(0.4226) | lr 9.9403e-04
06:45:17 01-24 [epoch 17 train] [step 150/193] [eta 0:00:01]: loss 0.4287(0.4363) | lr 9.9403e-04
06:45:20 01-24 [epoch 17 train] [step 193/193] [eta 0:00:00]: loss 0.4329(0.4330) | lr 9.9403e-04
06:45:24 01-24 [epoch 17 val]: Accuracy 67.7083 | Loss 0.5294
06:45:48 01-24 [epoch 18 train] [step 1/193] [eta 0:00:28]: loss 0.4843(0.4355) | lr 9.9367e-04
06:45:27 01-24 [epoch 18 train] [step 50/193] [eta 0:00:17]: loss 0.4955(0.4955) | lr 9.7626e-04
06:45:29 01-24 [epoch 18 train] [step 100/193] [eta 0:00:04]: loss 0.4506(0.4058) | lr 9.7626e-04
06:45:30 01-24 [epoch 18 train] [step 150/193] [eta 0:00:01]: loss 0.4424(0.4260) | lr 9.7626e-04
06:45:32 01-24 [epoch 18 train] [step 193/193] [eta 0:00:00]: loss 0.4439(0.4380) | lr 9.7626e-04
06:45:36 01-24 [epoch 18 val]: Accuracy 62.5000 | Loss 0.5945
06:45:36 01-24 [epoch 19 train] [step 1/193] [eta 0:00:23]: loss 0.5624(0.4457) | lr 9.7568e-04
06:45:38 01-24 [epoch 19 train] [step 50/193] [eta 0:00:17]: loss 0.4050(0.4050) | lr 9.4714e-04
06:45:41 01-24 [epoch 19 train] [step 100/193] [eta 0:00:06]: loss 0.4113(0.4176) | lr 9.4714e-04
06:45:43 01-24 [epoch 19 train] [step 150/193] [eta 0:00:01]: loss 0.4277(0.4606) | lr 9.4714e-04
06:45:44 01-24 [epoch 19 train] [step 193/193] [eta 0:00:00]: loss 0.4164(0.3694) | lr 9.4714e-04
06:45:48 01-24 [epoch 19 val]: Accuracy 69.7917 | Loss 0.5038
06:45:48 01-24 [epoch 20 train] [step 1/193] [eta 0:00:21]: loss 0.1629(0.3676) | lr 9.4634e-04
06:45:50 01-24 [epoch 20 train] [step 50/193] [eta 0:00:15]: loss 0.3612(0.3612) | lr 9.0737e-04
06:45:52 01-24 [epoch 20 train] [step 100/193] [eta 0:00:03]: loss 0.3676(0.3740) | lr 9.0737e-04
06:45:54 01-24 [epoch 20 train] [step 150/193] [eta 0:00:01]: loss 0.3829(0.4134) | lr 9.0737e-04
06:46:19 01-24 [epoch 20 train] [step 193/193] [eta 0:00:00]: loss 0.3846(0.3991) | lr 9.0737e-04
06:46:23 01-24 [epoch 20 val]: Accuracy 75.0000 | Loss 0.4429
06:46:23 01-24 [epoch 21 train] [step 1/193] [eta 0:00:22]: loss 0.2274(0.3956) | lr 9.0638e-04
06:46:24 01-24 [epoch 21 train] [step 50/193] [eta 0:00:16]: loss 0.4007(0.4007) | lr 8.5795e-04
06:46:26 01-24 [epoch 21 train] [step 100/193] [eta 0:00:02]: loss 0.3844(0.3681) | lr 8.5795e-04
06:46:27 01-24 [epoch 21 train] [step 150/193] [eta 0:00:01]: loss 0.4011(0.4344) | lr 8.5795e-04
06:46:29 01-24 [epoch 21 train] [step 193/193] [eta 0:00:00]: loss 0.3974(0.3892) | lr 8.5795e-04
06:46:32 01-24 [epoch 21 val]: Accuracy 73.9583 | Loss 0.4256
06:46:10 01-24 [epoch 22 train] [step 1/193] [eta 0:00:21]: loss 0.3676(0.3842) | lr 8.5679e-04
06:46:11 01-24 [epoch 22 train] [step 50/193] [eta 0:00:15]: loss 0.4017(0.4017) | lr 8.0008e-04
06:46:13 01-24 [epoch 22 train] [step 100/193] [eta 0:00:03]: loss 0.3858(0.3699) | lr 8.0008e-04
06:46:38 01-24 [epoch 22 train] [step 150/193] [eta 0:00:01]: loss 0.3821(0.3748) | lr 8.0008e-04
06:46:17 01-24 [epoch 22 train] [step 193/193] [eta 0:00:00]: loss 0.3865(0.3880) | lr 8.0008e-04
06:46:20 01-24 [epoch 22 val]: Accuracy 72.3958 | Loss 0.5053
06:46:21 01-24 [epoch 23 train] [step 1/193] [eta 0:00:23]: loss 0.6384(0.3966) | lr 7.9878e-04
06:46:22 01-24 [epoch 23 train] [step 50/193] [eta 0:00:15]: loss 0.3610(0.3610) | lr 7.3519e-04
06:46:24 01-24 [epoch 23 train] [step 100/193] [eta 0:00:02]: loss 0.3407(0.3205) | lr 7.3519e-04
06:46:25 01-24 [epoch 23 train] [step 150/193] [eta 0:00:01]: loss 0.3460(0.3566) | lr 7.3519e-04
06:46:27 01-24 [epoch 23 train] [step 193/193] [eta 0:00:00]: loss 0.3698(0.4458) | lr 7.3519e-04
06:46:31 01-24 [epoch 23 val]: Accuracy 71.8750 | Loss 0.4771
06:46:32 01-24 [epoch 24 train] [step 1/193] [eta 0:00:26]: loss 0.3044(0.4379) | lr 7.3378e-04
06:46:34 01-24 [epoch 24 train] [step 50/193] [eta 0:00:20]: loss 0.3096(0.3096) | lr 6.6487e-04
06:46:35 01-24 [epoch 24 train] [step 100/193] [eta 0:00:03]: loss 0.3103(0.3110) | lr 6.6487e-04
06:46:37 01-24 [epoch 24 train] [step 150/193] [eta 0:00:01]: loss 0.3227(0.3474) | lr 6.6487e-04
06:46:38 01-24 [epoch 24 train] [step 193/193] [eta 0:00:00]: loss 0.3246(0.3387) | lr 6.6487e-04
06:46:42 01-24 [epoch 24 val]: Accuracy 72.3958 | Loss 0.5794
06:46:42 01-24 [epoch 25 train] [step 1/193] [eta 0:00:21]: loss 0.2260(0.3336) | lr 6.6339e-04
06:46:44 01-24 [epoch 25 train] [step 50/193] [eta 0:00:15]: loss 0.3287(0.3287) | lr 5.9087e-04
06:46:46 01-24 [epoch 25 train] [step 100/193] [eta 0:00:03]: loss 0.3253(0.3219) | lr 5.9087e-04
06:46:47 01-24 [epoch 25 train] [step 150/193] [eta 0:00:01]: loss 0.3296(0.3381) | lr 5.9087e-04
06:46:49 01-24 [epoch 25 train] [step 193/193] [eta 0:00:00]: loss 0.3393(0.3720) | lr 5.9087e-04
06:46:53 01-24 [epoch 25 val]: Accuracy 69.7917 | Loss 0.4963
06:46:53 01-24 [epoch 26 train] [step 1/193] [eta 0:00:23]: loss 0.2853(0.3693) | lr 5.8935e-04
06:47:18 01-24 [epoch 26 train] [step 50/193] [eta 0:00:16]: loss 0.3033(0.3033) | lr 5.1500e-04
06:46:56 01-24 [epoch 26 train] [step 100/193] [eta 0:00:03]: loss 0.3247(0.3462) | lr 5.1500e-04
06:46:58 01-24 [epoch 26 train] [step 150/193] [eta 0:00:01]: loss 0.3291(0.3377) | lr 5.1500e-04
06:46:59 01-24 [epoch 26 train] [step 193/193] [eta 0:00:00]: loss 0.3346(0.3477) | lr 5.1500e-04
06:47:03 01-24 [epoch 26 val]: Accuracy 72.9167 | Loss 0.4748
06:47:04 01-24 [epoch 27 train] [step 1/193] [eta 0:00:22]: loss 0.1796(0.3467) | lr 5.1348e-04
06:47:05 01-24 [epoch 27 train] [step 50/193] [eta 0:00:16]: loss 0.3119(0.3119) | lr 4.3913e-04
06:47:07 01-24 [epoch 27 train] [step 100/193] [eta 0:00:03]: loss 0.3376(0.3633) | lr 4.3913e-04
06:47:08 01-24 [epoch 27 train] [step 150/193] [eta 0:00:01]: loss 0.3320(0.3209) | lr 4.3913e-04
06:47:10 01-24 [epoch 27 train] [step 193/193] [eta 0:00:00]: loss 0.3292(0.3194) | lr 4.3913e-04
06:47:14 01-24 [epoch 27 val]: Accuracy 75.5208 | Loss 0.4691
06:47:15 01-24 [epoch 28 train] [step 1/193] [eta 0:00:26]: loss 0.3255(0.3228) | lr 4.3765e-04
06:47:16 01-24 [epoch 28 train] [step 50/193] [eta 0:00:17]: loss 0.3156(0.3156) | lr 3.6513e-04
06:47:18 01-24 [epoch 28 train] [step 100/193] [eta 0:00:02]: loss 0.2849(0.2542) | lr 3.6513e-04
06:47:19 01-24 [epoch 28 train] [step 150/193] [eta 0:00:01]: loss 0.2880(0.2943) | lr 3.6513e-04
06:47:20 01-24 [epoch 28 train] [step 193/193] [eta 0:00:00]: loss 0.2991(0.3278) | lr 3.6513e-04
06:47:24 01-24 [epoch 28 val]: Accuracy 72.3958 | Loss 0.4869
06:47:24 01-24 [epoch 29 train] [step 1/193] [eta 0:00:19]: loss 0.3178(0.3300) | lr 3.6372e-04
06:47:25 01-24 [epoch 29 train] [step 50/193] [eta 0:00:14]: loss 0.2926(0.2926) | lr 2.9481e-04
06:47:27 01-24 [epoch 29 train] [step 100/193] [eta 0:00:02]: loss 0.2930(0.2934) | lr 2.9481e-04
06:47:28 01-24 [epoch 29 train] [step 150/193] [eta 0:00:01]: loss 0.2975(0.3066) | lr 2.9481e-04
06:47:29 01-24 [epoch 29 train] [step 193/193] [eta 0:00:00]: loss 0.2920(0.2760) | lr 2.9481e-04
06:47:33 01-24 [epoch 29 val]: Accuracy 73.4375 | Loss 0.4877
06:47:33 01-24 [epoch 30 train] [step 1/193] [eta 0:00:20]: loss 0.1858(0.2727) | lr 2.9352e-04
06:47:58 01-24 [epoch 30 train] [step 50/193] [eta 0:00:15]: loss 0.2729(0.2729) | lr 2.2992e-04
06:47:36 01-24 [epoch 30 train] [step 100/193] [eta 0:00:02]: loss 0.2872(0.3014) | lr 2.2992e-04
06:47:37 01-24 [epoch 30 train] [step 150/193] [eta 0:00:01]: loss 0.2879(0.2894) | lr 2.2992e-04
06:47:39 01-24 [epoch 30 train] [step 193/193] [eta 0:00:00]: loss 0.2957(0.3193) | lr 2.2992e-04
06:47:42 01-24 [epoch 30 val]: Accuracy 75.0000 | Loss 0.4944
06:47:43 01-24 [epoch 31 train] [step 1/193] [eta 0:00:21]: loss 0.1721(0.3117) | lr 2.2877e-04
06:47:44 01-24 [epoch 31 train] [step 50/193] [eta 0:00:15]: loss 0.3180(0.3180) | lr 1.7205e-04
06:47:45 01-24 [epoch 31 train] [step 100/193] [eta 0:00:02]: loss 0.3027(0.2874) | lr 1.7205e-04
06:47:47 01-24 [epoch 31 train] [step 150/193] [eta 0:00:01]: loss 0.3159(0.3423) | lr 1.7205e-04
06:47:48 01-24 [epoch 31 train] [step 193/193] [eta 0:00:00]: loss 0.3087(0.2831) | lr 1.7205e-04
06:47:51 01-24 [epoch 31 val]: Accuracy 75.0000 | Loss 0.5144
06:47:52 01-24 [epoch 32 train] [step 1/193] [eta 0:00:20]: loss 0.2255(0.2833) | lr 1.7106e-04
06:47:53 01-24 [epoch 32 train] [step 50/193] [eta 0:00:14]: loss 0.2992(0.2992) | lr 1.2263e-04
06:47:55 01-24 [epoch 32 train] [step 100/193] [eta 0:00:02]: loss 0.2922(0.2852) | lr 1.2263e-04
06:48:19 01-24 [epoch 32 train] [step 150/193] [eta 0:00:01]: loss 0.2884(0.2807) | lr 1.2263e-04
06:48:20 01-24 [epoch 32 train] [step 193/193] [eta 0:00:00]: loss 0.2871(0.2814) | lr 1.2263e-04
06:48:24 01-24 [epoch 32 val]: Accuracy 73.4375 | Loss 0.4626
06:48:24 01-24 [epoch 33 train] [step 1/193] [eta 0:00:21]: loss 0.2683(0.2835) | lr 1.2183e-04
06:48:26 01-24 [epoch 33 train] [step 50/193] [eta 0:00:15]: loss 0.3099(0.3099) | lr 8.2862e-05
06:48:27 01-24 [epoch 33 train] [step 100/193] [eta 0:00:02]: loss 0.2868(0.2638) | lr 8.2862e-05
06:48:28 01-24 [epoch 33 train] [step 150/193] [eta 0:00:01]: loss 0.2786(0.2621) | lr 8.2862e-05
06:48:30 01-24 [epoch 33 train] [step 193/193] [eta 0:00:00]: loss 0.2821(0.2815) | lr 8.2862e-05
06:48:10 01-24 [epoch 33 val]: Accuracy 74.4792 | Loss 0.4859
06:48:11 01-24 [epoch 34 train] [step 1/193] [eta 0:00:20]: loss 0.3878(0.2858) | lr 8.2279e-05
06:48:12 01-24 [epoch 34 train] [step 50/193] [eta 0:00:15]: loss 0.3212(0.3212) | lr 5.3738e-05
06:48:13 01-24 [epoch 34 train] [step 100/193] [eta 0:00:02]: loss 0.2986(0.2760) | lr 5.3738e-05
06:48:15 01-24 [epoch 34 train] [step 150/193] [eta 0:00:01]: loss 0.3043(0.3157) | lr 5.3738e-05
06:48:16 01-24 [epoch 34 train] [step 193/193] [eta 0:00:00]: loss 0.2997(0.2865) | lr 5.3738e-05
06:48:19 01-24 [epoch 34 val]: Accuracy 75.5208 | Loss 0.4941
06:48:43 01-24 [epoch 35 train] [step 1/193] [eta 0:00:20]: loss 0.1775(0.2860) | lr 5.3382e-05
06:48:21 01-24 [epoch 35 train] [step 50/193] [eta 0:00:15]: loss 0.2891(0.2891) | lr 3.5971e-05
06:48:23 01-24 [epoch 35 train] [step 100/193] [eta 0:00:02]: loss 0.2700(0.2510) | lr 3.5971e-05
06:48:24 01-24 [epoch 35 train] [step 150/193] [eta 0:00:01]: loss 0.3006(0.3618) | lr 3.5971e-05
06:48:25 01-24 [epoch 35 train] [step 193/193] [eta 0:00:00]: loss 0.2950(0.2754) | lr 3.5971e-05
06:48:29 01-24 [epoch 35 val]: Accuracy 73.9583 | Loss 0.4901
06:48:29 01-24 [epoch 36 train] [step 1/193] [eta 0:00:21]: loss 0.2098(0.2762) | lr 5.5252e-05
06:48:31 01-24 [epoch 36 train] [step 50/193] [eta 0:00:16]: loss 0.3008(0.3008) | lr 1.0000e-03
06:48:32 01-24 [epoch 36 train] [step 100/193] [eta 0:00:02]: loss 0.3008(0.3008) | lr 1.0000e-03
06:48:34 01-24 [epoch 36 train] [step 150/193] [eta 0:00:01]: loss 0.2924(0.2756) | lr 1.0000e-03
06:48:58 01-24 [epoch 36 train] [step 193/193] [eta 0:00:00]: loss 0.2869(0.2697) | lr 1.0000e-03
06:48:38 01-24 [epoch 36 val]: Accuracy 70.8333 | Loss 0.5294
06:48:39 01-24 [epoch 37 train] [step 1/193] [eta 0:00:21]: loss 0.2602(0.2700) | lr 9.9997e-04
06:48:40 01-24 [epoch 37 train] [step 50/193] [eta 0:00:15]: loss 0.3014(0.3014) | lr 9.9850e-04
06:48:42 01-24 [epoch 37 train] [step 100/193] [eta 0:00:02]: loss 0.3308(0.3602) | lr 9.9850e-04
06:48:43 01-24 [epoch 37 train] [step 150/193] [eta 0:00:01]: loss 0.3255(0.3148) | lr 9.9850e-04
06:48:44 01-24 [epoch 37 train] [step 193/193] [eta 0:00:00]: loss 0.3172(0.2950) | lr 9.9850e-04
06:48:48 01-24 [epoch 37 val]: Accuracy 72.3958 | Loss 0.5761
06:48:48 01-24 [epoch 38 train] [step 1/193] [eta 0:00:20]: loss 0.2653(0.2934) | lr 9.9842e-04
06:48:49 01-24 [epoch 38 train] [step 50/193] [eta 0:00:15]: loss 0.2543(0.2543) | lr 9.9403e-04
06:48:51 01-24 [epoch 38 train] [step 100/193] [eta 0:00:02]: loss 0.2606(0.2670) | lr 9.9403e-04
06:48:52 01-24 [epoch 38 train] [step 150/193] [eta 0:00:01]: loss 0.2681(0.2830) | lr 9.9403e-04
06:48:54 01-24 [epoch 38 train] [step 193/193] [eta 0:00:00]: loss 0.2710(0.2783) | lr 9.9403e-04
06:48:57 01-24 [epoch 38 val]: Accuracy 69.2708 | Loss 0.5547
06:48:58 01-24 [epoch 39 train] [step 1/193] [eta 0:00:23]: loss 0.1753(0.2781) | lr 9.9388e-04
06:49:00 01-24 [epoch 39 train] [step 50/193] [eta 0:00:17]: loss 0.3217(0.3217) | lr 9.8660e-04
06:49:01 01-24 [epoch 39 train] [step 100/193] [eta 0:00:02]: loss 0.2971(0.2724) | lr 9.8660e-04
06:49:02 01-24 [epoch 39 train] [step 150/193] [eta 0:00:01]: loss 0.2924(0.2830) | lr 9.8660e-04
06:49:03 01-24 [epoch 39 train] [step 193/193] [eta 0:00:00]: loss 0.2946(0.2960) | lr 9.8660e-04
06:49:07 01-24 [epoch 39 val]: Accuracy 69.7917 | Loss 0.5794
06:49:08 01-24 [epoch 40 train] [step 1/193] [eta 0:00:22]: loss 0.2169(0.2967) | lr 9.8639e-04
06:49:09 01-24 [epoch 40 train] [step 50/193] [eta 0:00:16]: loss 0.2845(0.2845) | lr 9.7626e-04
06:49:10 01-24 [epoch 40 train] [step 100/193] [eta 0:00:02]: loss 0.2713(0.2581) | lr 9.7626e-04
06:49:12 01-24 [epoch 40 train] [step 150/193] [eta 0:00:01]: loss 0.2691(0.2648) | lr 9.7626e-04
06:49:13 01-24 [epoch 40 train] [step 193/193] [eta 0:00:00]: loss 0.2628(0.2477) | lr 9.7626e-04
06:49:17 01-24 [epoch 40 val]: Accuracy 74.4792 | Loss 0.7653
06:49:17 01-24 [epoch 41 train] [step 1/193] [eta 0:00:21]: loss 0.2950(0.2415) | lr 9.7600e-04
06:49:19 01-24 [epoch 41 train] [step 50/193] [eta 0:00:15]: loss 0.2784(0.2784) | lr 9.6308e-04
06:49:20 01-24 [epoch 41 train] [step 100/193] [eta 0:00:02]: loss 0.2602(0.2419) | lr 9.6308e-04
06:49:21 01-24 [epoch 41 train] [step 150/193] [eta 0:00:01]: loss 0.2580(0.2538) | lr 9.6308e-04
06:49:22 01-24 [epoch 41 train] [step 193/193] [eta 0:00:00]: loss 0.2641(0.2708) | lr 9.6308e-04
06:49:26 01-24 [epoch 41 val]: Accuracy 72.9167 | Loss 0.7758
06:49:26 01-24 [epoch 42 train] [step 1/193] [eta 0:00:19]: loss 0.1539(0.2706) | lr 9.6276e-04
06:49:27 01-24 [epoch 42 train] [step 50/193] [eta 0:00:15]: loss 0.2182(0.2182) | lr 9.4714e-04
06:49:28 01-24 [epoch 42 train] [step 100/193] [eta 0:00:02]: loss 0.2504(0.2826) | lr 9.4714e-04
06:49:30 01-24 [epoch 42 train] [step 150/193] [eta 0:00:01]: loss 0.2563(0.2680) | lr 9.4714e-04
06:49:31 01-24 [epoch 42 train] [step 193/193] [eta 0:00:00]: loss 0.2643(0.2917) | lr 9.4714e-04
06:49:34 01-24 [epoch 42 val]: Accuracy 75.0000 | Loss 0.6754
06:49:35 01-24 [epoch 43 train] [step 1/193] [eta 0:00:20]: loss 0.1641(0.2893) | lr 9.4677e-04
06:49:36 01-24 [epoch 43 train] [step 50/193] [eta 0:00:15]: loss 0.2429(0.2429) | lr 9.2853e-04
06:49:37 01-24 [epoch 43 train] [step 100/193] [eta 0:00:02]: loss 0.2406(0.2384) | lr 9.2853e-04
06:49:39 01-24 [epoch 43 train] [step 150/193] [eta 0:00:01]: loss 0.2442(0.2514) | lr 9.2853e-04
06:49:40 01-24 [epoch 43 train] [step 193/193] [eta 0:00:00]: loss 0.2378(0.2139) | lr 9.2853e-04
06:49:43 01-24 [epoch 43 val]: Accuracy 70.3125 | Loss 0.6500
06:49:44 01-24 [epoch 44 train] [step 1/193] [eta 0:00:19]: loss 0.2100(0.2136) | lr 9.2811e-04
06:50:08 01-24 [epoch 44 train] [step 50/193] [eta 0:00:14]: loss 0.2189(0.2189) | lr 9.0737e-04
06:49:46 01-24 [epoch 44 train] [step 100/193] [eta 0:00:02]: loss 0.2167(0.2146) | lr 9.0737e-04
06:49:47 01-24 [epoch 44 train] [step 150/193] [eta 0:00:01]: loss 0.2403(0.2875) | lr 9.0737e-04
06:49:48 01-24 [epoch 44 train] [step 193/193] [eta 0:00:00]: loss 0.2429(0.2735) | lr 9.0737e-04
06:49:52 01-24 [epoch 44 val]: Accuracy 72.3958 | Loss 0.6371
06:49:52 01-24 [epoch 45 train] [step 1/193] [eta 0:00:19]: loss 0.5124(0.2738) | lr 9.0690e-04
06:49:53 01-24 [epoch 45 train] [step 50/193] [eta 0:00:14]: loss 0.2355(0.2355) | lr 8.8380e-04
06:49:55 01-24 [epoch 45 train] [step 100/193] [eta 0:00:02]: loss 0.2301(0.2247) | lr 8.8380e-04
06:49:56 01-24 [epoch 45 train] [step 150/193] [eta 0:00:01]: loss 0.2367(0.2499) | lr 8.8380e-04
06:49:57 01-24 [epoch 45 train] [step 193/193] [eta 0:00:00]: loss 0.2409(0.2461) | lr 8.8380e-04
06:50:01 01-24 [epoch 45 val]: Accuracy 73.9583 | Loss 0.7097
06:50:01 01-24 [epoch 46 train] [step 1/193] [eta 0:00:20]: loss 0.7607(0.2579) | lr 8.8328e-04
06:50:02 01-24 [epoch 46 train] [step 50/193] [eta 0:00:15]: loss 0.2294(0.2294) | lr 8.5795e-04
06:50:03 01-24 [epoch 46 train] [step 100/193] [eta 0:00:02]: loss 0.2379(0.2463) | lr 8.5795e-04
06:50:28 01-24 [epoch 46 train] [step 150/193] [eta 0:00:01]: loss 0.2413(0.2483) | lr 8.5795e-04
06:50:06 01-24 [epoch 46 train] [step 193/193] [eta 0:00:00]: loss 0.2332(0.2153) | lr 8.5795e-04
06:50:10 01-24 [epoch 46 val]: Accuracy 70.8333 | Loss 0.5275
06:50:10 01-24 [epoch 47 train] [step 1/193] [eta 0:00:20]: loss 0.2433(0.2169) | lr 8.5739e-04
06:50:11 01-24 [epoch 47 train] [step 50/193] [eta 0:00:15]: loss 0.2188(0.2188) | lr 8.2998e-04
06:50:13 01-24 [epoch 47 train] [step 100/193] [eta 0:00:02]: loss 0.2166(0.2145) | lr 8.2998e-04
06:50:14 01-24 [epoch 47 train] [step 150/193] [eta 0:00:01]: loss 0.2292(0.2543) | lr 8.2998e-04
06:50:38 01-24 [epoch 47 train] [step 193/193] [eta 0:00:00]: loss 0.2224(0.2040) | lr 8.2998e-04
06:50:19 01-24 [epoch 47 val]: Accuracy 73.9583 | Loss 0.6350
06:50:19 01-24 [epoch 48 train] [step 1/193] [eta 0:00:20]: loss 0.1752(0.2038) | lr 8.2938e-04
06:50:20 01-24 [epoch 48 train] [step 50/193] [eta 0:00:15]: loss 0.2020(0.2020) | lr 8.0008e-04
06:50:22 01-24 [epoch 48 train] [step 100/193] [eta 0:00:02]: loss 0.2057(0.2094) | lr 8.0008e-04
06:50:23 01-24 [epoch 48 train] [step 150/193] [eta 0:00:01]: loss 0.2067(0.2086) | lr 8.0008e-04
06:50:24 01-24 [epoch 48 train] [step 193/193] [eta 0:00:00]: loss 0.2066(0.2014) | lr 8.0008e-04
06:50:27 01-24 [epoch 48 val]: Accuracy 76.0417 | Loss 0.7829
06:50:28 01-24 [epoch 49 train] [step 1/193] [eta 0:00:20]: loss 0.3670(0.2051) | lr 7.9944e-04
06:50:29 01-24 [epoch 49 train] [step 50/193] [eta 0:00:15]: loss 0.2205(0.2205) | lr 7.6841e-04
06:50:30 01-24 [epoch 49 train] [step 100/193] [eta 0:00:02]: loss 0.2246(0.2286) | lr 7.6841e-04
06:50:31 01-24 [epoch 49 train] [step 150/193] [eta 0:00:01]: loss 0.2283(0.2359) | lr 7.6841e-04
06:50:33 01-24 [epoch 49 train] [step 193/193] [eta 0:00:00]: loss 0.2242(0.2089) | lr 7.6841e-04
06:50:36 01-24 [epoch 49 val]: Accuracy 75.0000 | Loss 0.8238
06:50:37 01-24 [epoch 50 train] [step 1/193] [eta 0:00:20]: loss 0.4245(0.2143) | lr 7.6775e-04
06:50:38 01-24 [epoch 50 train] [step 50/193] [eta 0:00:15]: loss 0.1952(0.1952) | lr 7.3519e-04
06:50:39 01-24 [epoch 50 train] [step 100/193] [eta 0:00:02]: loss 0.2151(0.2350) | lr 7.3519e-04
06:50:40 01-24 [epoch 50 train] [step 150/193] [eta 0:00:01]: loss 0.2213(0.2336) | lr 7.3519e-04
06:50:41 01-24 [epoch 50 train] [step 193/193] [eta 0:00:00]: loss 0.2216(0.2336) | lr 7.3519e-04
06:51:08 01-24 [epoch 50 val]: Accuracy 71.8750 | Loss 0.6590
06:50:45 01-24 [epoch 51 train] [step 1/193] [eta 0:00:19]: loss 0.1831(0.2297) | lr 7.3449e-04
06:50:47 01-24 [epoch 51 train] [step 50/193] [eta 0:00:14]: loss 0.2196(0.2196) | lr 7.0060e-04
06:50:48 01-24 [epoch 51 train] [step 100/193] [eta 0:00:02]: loss 0.2045(0.1893) | lr 7.0060e-04
06:50:49 01-24 [epoch 51 train] [step 150/193] [eta 0:00:01]: loss 0.2057(0.2082) | lr 7.0060e-04
06:51:13 01-24 [epoch 51 train] [step 193/193] [eta 0:00:00]: loss 0.2002(0.1893) | lr 7.0060e-04
06:50:54 01-24 [epoch 51 val]: Accuracy 74.4792 | Loss 0.6571
06:50:54 01-24 [epoch 52 train] [step 1/193] [eta 0:00:20]: loss 0.1534(0.1851) | lr 6.9989e-04
06:50:55 01-24 [epoch 52 train] [step 50/193] [eta 0:00:15]: loss 0.2136(0.2136) | lr 6.6487e-04
06:50:57 01-24 [epoch 52 train] [step 100/193] [eta 0:00:02]: loss 0.2091(0.2046) | lr 6.6487e-04
06:50:58 01-24 [epoch 52 train] [step 150/193] [eta 0:00:01]: loss 0.2104(0.2129) | lr 6.6487e-04
06:50:59 01-24 [epoch 52 train] [step 193/193] [eta 0:00:00]: loss 0.2141(0.2253) | lr 6.6487e-04
06:51:02 01-24 [epoch 52 val]: Accuracy 74.4792 | Loss 0.8775
06:51:03 01-24 [epoch 53 train] [step 1/193] [eta 0:00:20]: loss 0.2260(0.2262) | lr 6.6414e-04
06:51:04 01-24 [epoch 53 train] [step 50/193] [eta 0:00:15]: loss 0.1965(0.1965) | lr 6.2822e-04
06:51:05 01-24 [epoch 53 train] [step 100/193] [eta 0:00:02]: loss 0.1973(0.1981) | lr 6.2822e-04
06:51:06 01-24 [epoch 53 train] [step 150/193] [eta 0:00:01]: loss 0.1978(0.1988) | lr 6.2822e-04
06:51:07 01-24 [epoch 53 train] [step 193/193] [eta 0:00:00]: loss 0.2013(0.2109) | lr 6.2822e-04
06:51:11 01-24 [epoch 53 val]: Accuracy 72.9167 | Loss 0.6720
06:51:12 01-24 [epoch 54 train] [step 1/193] [eta 0:00:20]: loss 0.1557(0.2110) | lr 6.2747e-04
06:51:13 01-24 [epoch 54 train] [step 50/193] [eta 0:00:15]: loss 0.2003(0.2003) | lr 5.9087e-04
06:51:14 01-24 [epoch 54 train] [step 100/193] [eta 0:00:02]: loss 0.2037(0.2070) | lr 5.9087e-04
06:51:15 01-24 [epoch 54 train] [step 150/193] [eta 0:00:01]: loss 0.2091(0.2200) | lr 5.9087e-04
06:51:17 01-24 [epoch 54 train] [step 193/193] [eta 0:00:00]: loss 0.2057(0.1898) | lr 5.9087e-04
06:51:20 01-24 [epoch 54 val]: Accuracy 72.3958 | Loss 0.6283
06:51:21 01-24 [epoch 55 train] [step 1/193] [eta 0:00:22]: loss 0.1580(0.1897) | lr 5.9011e-04
06:51:23 01-24 [epoch 55 train] [step 50/193] [eta 0:00:15]: loss 0.2019(0.2019) | lr 5.5305e-04
06:51:24 01-24 [epoch 55 train] [step 100/193] [eta 0:00:03]: loss 0.2055(0.2090) | lr 5.5305e-04
06:51:26 01-24 [epoch 55 train] [step 150/193] [eta 0:00:01]: loss 0.2156(0.2358) | lr 5.5305e-04
06:51:28 01-24 [epoch 55 train] [step 193/193] [eta 0:00:00]: loss 0.2128(0.2136) | lr 5.5305e-04
06:51:32 01-24 [epoch 55 val]: Accuracy 73.4375 | Loss 0.8244
06:51:32 01-24 [epoch 56 train] [step 1/193] [eta 0:00:24]: loss 0.1746(0.2014) | lr 5.5229e-04
06:51:34 01-24 [epoch 56 train] [step 50/193] [eta 0:00:16]: loss 0.2423(0.2423) | lr 5.1500e-04
06:51:35 01-24 [epoch 56 train] [step 100/193] [eta 0:00:02]: loss 0.2127(0.1831) | lr 5.1500e-04
06:51:37 01-24 [epoch 56 train] [step 150/193] [eta 0:00:01]: loss 0.2211(0.2380) | lr 5.1500e-04
06:51:38 01-24 [epoch 56 train] [step 193/193] [eta 0:00:00]: loss 0.2275(0.2520) | lr 5.1500e-04
06:51:42 01-24 [epoch 56 val]: Accuracy 73.9583 | Loss 0.8227
06:51:43 01-24 [epoch 57 train] [step 1/193] [eta 0:00:22]: loss 0.1892(0.2523) | lr 5.1424e-04
06:51:44 01-24 [epoch 57 train] [step 50/193] [eta 0:00:17]: loss 0.1888(0.1888) | lr 4.7695e-04
06:51:46 01-24 [epoch 57 train] [step 100/193] [eta 0:00:02]: loss 0.1853(0.1819) | lr 4.7695e-04
06:51:47 01-24 [epoch 57 train] [step 150/193] [eta 0:00:01]: loss 0.1906(0.2012) | lr 4.7695e-04
06:51:49 01-24 [epoch 57 train] [step 193/193] [eta 0:00:00]: loss 0.1975(0.2270) | lr 4.7695e-04
06:51:53 01-24 [epoch 57 val]: Accuracy 74.4792 | Loss 0.7854
06:51:53 01-24 [epoch 58 train] [step 1/193] [eta 0:00:23]: loss 0.1539(0.2230) | lr 4.7619e-04
06:51:55 01-24 [epoch 58 train] [step 50/193] [eta 0:00:16]: loss 0.2051(0.2051) | lr 4.3913e-04
06:51:56 01-24 [epoch 58 train] [step 100/193] [eta 0:00:03]: loss 0.2001(0.1952) | lr 4.3913e-04
06:51:58 01-24 [epoch 58 train] [step 150/193] [eta 0:00:01]: loss 0.2017(0.2048) | lr 4.3913e-04
06:52:00 01-24 [epoch 58 train] [step 193/193] [eta 0:00:00]: loss 0.2008(0.1999) | lr 4.3913e-04
06:52:04 01-24 [epoch 58 val]: Accuracy 74.4792 | Loss 0.7503
06:52:05 01-24 [epoch 59 train] [step 1/193] [eta 0:00:29]: loss 0.1822(0.2000) | lr 4.3838e-04
06:52:09 01-24 [epoch 59 train] [step 50/193] [eta 0:00:25]: loss 0.2022(0.2022) | lr 4.0178e-04
06:52:33 01-24 [epoch 59 train] [step 100/193] [eta 0:00:02]: loss 0.2027(0.2031) | lr 4.0178e-04
06:52:12 01-24 [epoch 59 train] [step 150/193] [eta 0:00:01]: loss 0.1962(0.1834) | lr 4.0178e-04
06:52:13 01-24 [epoch 59 train] [step 193/193] [eta 0:00:00]: loss 0.1950(0.1908) | lr 4.0178e-04
06:52:17 01-24 [epoch 59 val]: Accuracy 71.8750 | Loss 0.6711
06:52:17 01-24 [epoch 60 train] [step 1/193] [eta 0:00:22]: loss 0.4516(0.1960) | lr 4.0105e-04
06:52:19 01-24 [epoch 60 train] [step 50/193] [eta 0:00:16]: loss 0.2127(0.2127) | lr 3.6513e-04
06:52:20 01-24 [epoch 60 train] [step 100/193] [eta 0:00:03]: loss 0.1976(0.1826) | lr 3.6513e-04
06:52:24 01-24 [epoch 60 train] [step 150/193] [eta 0:00:02]: loss 0.2017(0.2099) | lr 3.6513e-04
06:52:48 01-24 [epoch 60 train] [step 193/193] [eta 0:00:00]: loss 0.2067(0.2150) | lr 3.6513e-04
06:52:29 01-24 [epoch 60 val]: Accuracy 73.9583 | Loss 0.6657
06:52:30 01-24 [epoch 61 train] [step 1/193] [eta 0:00:24]: loss 0.1562(0.2151) | lr 3.6441e-04
06:52:31 01-24 [epoch 61 train] [step 50/193] [eta 0:00:17]: loss 0.1890(0.1890) | lr 3.2940e-04
06:52:33 01-24 [epoch 61 train] [step 100/193] [eta 0:00:02]: loss 0.1841(0.1791) | lr 3.2940e-04
06:52:34 01-24 [epoch 61 train] [step 150/193] [eta 0:00:01]: loss 0.1913(0.2057) | lr 3.2940e-04
06:52:36 01-24 [epoch 61 train] [step 193/193] [eta 0:00:00]: loss 0.1907(0.1954) | lr 3.2940e-04
06:52:39 01-24 [epoch 61 val]: Accuracy 73.9583 | Loss 0.7027
06:53:03 01-24 [epoch 62 train] [step 1/193] [eta 0:00:22]: loss 0.2674(0.1965) | lr 3.2871e-04
06:52:42 01-24 [epoch 62 train] [step 50/193] [eta 0:00:19]: loss 0.1944(0.1944) | lr 2.9481e-04
06:52:45 01-24 [epoch 62 train] [step 100/193] [eta 0:00:04]: loss 0.1963(0.1982) | lr 2.9481e-04
06:52:48 01-24 [epoch 62 train] [step 150/193] [eta 0:00:02]: loss 0.1939(0.1891) | lr 2.9481e-04
06:52:49 01-24 [epoch 62 train] [step 193/193] [eta 0:00:00]: loss 0.2006(0.2159) | lr 2.9481e-04
06:52:53 01-24 [epoch 62 val]: Accuracy 76.5625 | Loss 0.7017
06:52:54 01-24 [epoch 63 train] [step 1/193] [eta 0:00:23]: loss 0.1594(0.2159) | lr 2.9415e-04
06:52:56 01-24 [epoch 63 train] [step 50/193] [eta 0:00:19]: loss 0.1821(0.1821) | lr 2.6159e-04
06:52:58 01-24 [epoch 63 train] [step 100/193] [eta 0:00:03]: loss 0.1782(0.1742) | lr 2.6159e-04
06:52:59 01-24 [epoch 63 train] [step 150/193] [eta 0:00:01]: loss 0.1812(0.1872) | lr 2.6159e-04
06:53:01 01-24 [epoch 63 train] [step 193/193] [eta 0:00:00]: loss 0.1792(0.1724) | lr 2.6159e-04
06:53:04 01-24 [epoch 63 val]: Accuracy 73.9583 | Loss 0.7185
06:53:05 01-24 [epoch 64 train] [step 1/193] [eta 0:00:21]: loss 0.1537(0.1718) | lr 2.6095e-04
06:53:06 01-24 [epoch 64 train] [step 50/193] [eta 0:00:16]: loss 0.1903(0.1903) | lr 2.2992e-04
06:53:08 01-24 [epoch 64 train] [step 100/193] [eta 0:00:03]: loss 0.1873(0.1842) | lr 2.2992e-04
06:53:33 01-24 [epoch 64 train] [step 150/193] [eta 0:00:01]: loss 0.1999(0.2252) | lr 2.2992e-04
06:53:11 01-24 [epoch 64 train] [step 193/193] [eta 0:00:00]: loss 0.1943(0.1830) | lr 2.2992e-04
06:53:15 01-24 [epoch 64 val]: Accuracy 74.4792 | Loss 0.8526
06:53:16 01-24 [epoch 65 train] [step 1/193] [eta 0:00:21]: loss 0.2046(0.1748) | lr 2.2933e-04
06:53:17 01-24 [epoch 65 train] [step 50/193] [eta 0:00:15]: loss 0.1936(0.1936) | lr 2.0002e-04
06:53:19 01-24 [epoch 65 train] [step 100/193] [eta 0:00:04]: loss 0.1965(0.1994) | lr 2.0002e-04
06:53:22 01-24 [epoch 65 train] [step 150/193] [eta 0:00:02]: loss 0.1895(0.1756) | lr 2.0002e-04
06:53:25 01-24 [epoch 65 train] [step 193/193] [eta 0:00:00]: loss 0.1896(0.1866) | lr 2.0002e-04
06:53:29 01-24 [epoch 65 val]: Accuracy 74.4792 | Loss 0.8003
06:53:53 01-24 [epoch 66 train] [step 1/193] [eta 0:00:34]: loss 0.2483(0.1884) | lr 1.9946e-04
06:53:32 01-24 [epoch 66 train] [step 50/193] [eta 0:00:22]: loss 0.1837(0.1837) | lr 1.7205e-04
06:53:34 01-24 [epoch 66 train] [step 100/193] [eta 0:00:03]: loss 0.1789(0.1741) | lr 1.7205e-04
06:53:36 01-24 [epoch 66 train] [step 150/193] [eta 0:00:01]: loss 0.1818(0.1877) | lr 1.7205e-04
06:53:37 01-24 [epoch 66 train] [step 193/193] [eta 0:00:00]: loss 0.1789(0.1822) | lr 1.7205e-04
06:53:41 01-24 [epoch 66 val]: Accuracy 74.4792 | Loss 0.7851
06:53:42 01-24 [epoch 67 train] [step 1/193] [eta 0:00:23]: loss 0.1540(0.1821) | lr 1.7154e-04
06:53:43 01-24 [epoch 67 train] [step 50/193] [eta 0:00:18]: loss 0.1910(0.1910) | lr 1.4620e-04
06:53:45 01-24 [epoch 67 train] [step 100/193] [eta 0:00:02]: loss 0.1893(0.1875) | lr 1.4620e-04
06:53:46 01-24 [epoch 67 train] [step 150/193] [eta 0:00:01]: loss 0.1894(0.1897) | lr 1.4620e-04
06:53:48 01-24 [epoch 67 train] [step 193/193] [eta 0:00:00]: loss 0.1883(0.1840) | lr 1.4620e-04
06:53:51 01-24 [epoch 67 val]: Accuracy 75.0000 | Loss 0.7766
06:53:52 01-24 [epoch 68 train] [step 1/193] [eta 0:00:22]: loss 0.1561(0.1840) | lr 1.4573e-04
06:53:54 01-24 [epoch 68 train] [step 50/193] [eta 0:00:16]: loss 0.1670(0.1670) | lr 1.2263e-04
06:53:55 01-24 [epoch 68 train] [step 100/193] [eta 0:00:02]: loss 0.1658(0.1646) | lr 1.2263e-04
06:53:57 01-24 [epoch 68 train] [step 150/193] [eta 0:00:01]: loss 0.1704(0.1797) | lr 1.2263e-04
06:53:59 01-24 [epoch 68 train] [step 193/193] [eta 0:00:00]: loss 0.1760(0.1977) | lr 1.2263e-04
06:54:03 01-24 [epoch 68 val]: Accuracy 74.4792 | Loss 0.7542
06:54:03 01-24 [epoch 69 train] [step 1/193] [eta 0:00:24]: loss 0.1555(0.1977) | lr 1.2220e-04
06:54:05 01-24 [epoch 69 train] [step 50/193] [eta 0:00:17]: loss 0.1843(0.1843) | lr 1.0147e-04
06:54:06 01-24 [epoch 69 train] [step 100/193] [eta 0:00:02]: loss 0.1824(0.1806) | lr 1.0147e-04
06:54:08 01-24 [epoch 69 train] [step 150/193] [eta 0:00:01]: loss 0.1834(0.1854) | lr 1.0147e-04
06:54:09 01-24 [epoch 69 train] [step 193/193] [eta 0:00:00]: loss 0.1858(0.1910) | lr 1.0147e-04
06:54:13 01-24 [epoch 69 val]: Accuracy 75.5208 | Loss 0.7960
06:54:13 01-24 [epoch 70 train] [step 1/193] [eta 0:00:21]: loss 0.1545(0.1891) | lr 1.0110e-04
06:54:15 01-24 [epoch 70 train] [step 50/193] [eta 0:00:15]: loss 0.1780(0.1780) | lr 8.2862e-05
06:54:17 01-24 [epoch 70 train] [step 100/193] [eta 0:00:03]: loss 0.1805(0.1831) | lr 8.2862e-05
06:54:18 01-24 [epoch 70 train] [step 150/193] [eta 0:00:01]: loss 0.1796(0.1779) | lr 8.2862e-05
06:54:20 01-24 [epoch 70 train] [step 193/193] [eta 0:00:00]: loss 0.1868(0.2043) | lr 8.2862e-05
06:54:23 01-24 [epoch 70 val]: Accuracy 74.4792 | Loss 0.7693
06:54:24 01-24 [epoch 71 train] [step 1/193] [eta 0:00:21]: loss 0.1575(0.2040) | lr 8.2543e-05
06:54:48 01-24 [epoch 71 train] [step 50/193] [eta 0:00:16]: loss 0.1968(0.1968) | lr 6.6918e-05
06:54:27 01-24 [epoch 71 train] [step 100/193] [eta 0:00:02]: loss 0.1892(0.1815) | lr 6.6918e-05
06:54:28 01-24 [epoch 71 train] [step 150/193] [eta 0:00:01]: loss 0.1941(0.2038) | lr 6.6918e-05
06:54:30 01-24 [epoch 71 train] [step 193/193] [eta 0:00:00]: loss 0.1894(0.1895) | lr 6.6918e-05
06:54:34 01-24 [epoch 71 val]: Accuracy 75.0000 | Loss 0.8247
06:54:35 01-24 [epoch 72 train] [step 1/193] [eta 0:00:25]: loss 0.1543(0.1730) | lr 6.6655e-05
06:54:37 01-24 [epoch 72 train] [step 50/193] [eta 0:00:19]: loss 0.1936(0.1936) | lr 5.3738e-05
06:54:38 01-24 [epoch 72 train] [step 100/193] [eta 0:00:03]: loss 0.1873(0.1810) | lr 5.3738e-05
06:54:40 01-24 [epoch 72 train] [step 150/193] [eta 0:00:01]: loss 0.1857(0.1825) | lr 5.3738e-05
06:54:41 01-24 [epoch 72 train] [step 193/193] [eta 0:00:00]: loss 0.1915(0.2042) | lr 5.3738e-05
06:54:45 01-24 [epoch 72 val]: Accuracy 75.5208 | Loss 0.7963
06:54:45 01-24 [epoch 73 train] [step 1/193] [eta 0:00:20]: loss 0.1563(0.2040) | lr 5.3531e-05
06:54:47 01-24 [epoch 73 train] [step 50/193] [eta 0:00:16]: loss 0.2033(0.2033) | lr 4.3401e-05
06:54:49 01-24 [epoch 73 train] [step 100/193] [eta 0:00:02]: loss 0.1859(0.1685) | lr 4.3401e-05
06:54:50 01-24 [epoch 73 train] [step 150/193] [eta 0:00:01]: loss 0.1891(0.1956) | lr 4.3401e-05
06:54:51 01-24 [epoch 73 train] [step 193/193] [eta 0:00:00]: loss 0.1837(0.1691) | lr 4.3401e-05
06:54:55 01-24 [epoch 73 val]: Accuracy 75.0000 | Loss 0.7864
06:54:56 01-24 [epoch 74 train] [step 1/193] [eta 0:00:21]: loss 0.1826(0.1685) | lr 4.3252e-05
06:54:57 01-24 [epoch 74 train] [step 50/193] [eta 0:00:15]: loss 0.1938(0.1938) | lr 3.5971e-05
06:54:58 01-24 [epoch 74 train] [step 100/193] [eta 0:00:02]: loss 0.1929(0.1921) | lr 3.5971e-05
06:55:00 01-24 [epoch 74 train] [step 150/193] [eta 0:00:01]: loss 0.1903(0.1849) | lr 3.5971e-05
06:55:01 01-24 [epoch 74 train] [step 193/193] [eta 0:00:00]: loss 0.1898(0.1876) | lr 3.5971e-05
06:55:05 01-24 [epoch 74 val]: Accuracy 75.0000 | Loss 0.7723
06:55:06 01-24 [epoch 75 train] [step 1/193] [eta 0:00:22]: loss 0.1580(0.1875) | lr 3.5882e-05
06:55:07 01-24 [epoch 75 train] [step 50/193] [eta 0:00:16]: loss 0.1704(0.1704) | lr 3.1495e-05
06:55:09 01-24 [epoch 75 train] [step 100/193] [eta 0:00:03]: loss 0.1865(0.2025) | lr 3.1495e-05
06:55:11 01-24 [epoch 75 train] [step 150/193] [eta 0:00:01]: loss 0.1878(0.1904) | lr 3.1495e-05
06:55:13 01-24 [epoch 75 train] [step 193/193] [eta 0:00:00]: loss 0.1894(0.1901) | lr 3.1495e-05
06:55:17 01-24 [epoch 75 val]: Accuracy 75.5208 | Loss 0.7800
