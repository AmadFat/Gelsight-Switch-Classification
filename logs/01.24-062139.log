Log file created at 06:21:42 01-24 with path: logs/01.24-06:21:39.log
System information:
+-----------------------+------------------------------------+
|    CPU Physical Cores |                                 16 |
+-----------------------+------------------------------------+
|     CPU Logical Cores |                                 32 |
+-----------------------+------------------------------------+
|     CPU Max Frequency |                           0.00 MHz |
+-----------------------+------------------------------------+
| CPU Current Frequency |                        2419.20 MHz |
+-----------------------+------------------------------------+
|             RAM Total |                           10.70 GB |
+-----------------------+------------------------------------+
|         RAM Available |                            7.61 GB |
+-----------------------+------------------------------------+
|              RAM Used |                            2.79 GB |
+-----------------------+------------------------------------+
|            GPU Device | NVIDIA GeForce RTX 4060 Laptop GPU |
+-----------------------+------------------------------------+
|             GPU Count |                                  1 |
+-----------------------+------------------------------------+
|      GPU Memory Total |                            8.00 GB |
+-----------------------+------------------------------------+
|   GPU Memory Reserved |                            0.00 GB |
+-----------------------+------------------------------------+
|          CUDA Version |                               12.1 |
+-----------------------+------------------------------------+
|              Hostname |                            AmadFat |
+-----------------------+------------------------------------+
|                    OS |                              Linux |
+-----------------------+------------------------------------+
|            OS Release | 5.15.167.4-microsoft-standard-WSL2 |
+-----------------------+------------------------------------+
|            OS Version | #1 SMP Tue Nov 5 00:21:55 UTC 2024 |
+-----------------------+------------------------------------+
|        Python Version |                             3.9.21 |
+-----------------------+------------------------------------+
|       PyTorch Version |                        2.4.1+cu121 |
+-----------------------+------------------------------------+
|   TorchVision Version |                       0.19.1+cu121 |
+-----------------------+------------------------------------+
|    TorchAudio Version |                        2.4.1+cu121 |
+-----------------------+------------------------------------+
Experiment settings:
{'criterion': {'alpha': 5.0, 'criterion_name': 'focalloss', 'gamma': 1.5},
 'data': {'batch_size': 4,
          'num_workers': 4,
          'root': 'dataset',
          'split_ratio': [0.8, 0.2]},
 'evaluator': {'acc': True, 'loss': True},
 'experiment': {'ckpt_save_dir': PosixPath('ckpts/01.24-06:21:39'),
                'deterministic': False,
                'device': 'cuda',
                'experiment_name': '01.24-06:21:39',
                'max_epochs': 75,
                'save': True,
                'seed': 3407,
                'val': True,
                'val_interval': 1},
 'logger': {'log_save_path': PosixPath('logs/01.24-06:21:39.log'),
            'tb_save_path': PosixPath('tbevents/01.24-06:21:39'),
            'train_print_interval': 50,
            'use_tensorboard': True,
            'window_metric': 50,
            'window_time_stamp': 50},
 'model': {'last_fc_bias_init': 'const',
           'last_fc_weight_init': 'const',
           'model_name': 'resnet18',
           'norm_layer': 'batchnorm',
           'weights': 'default'},
 'optimizer': {'grad_clip': 1.0,
               'lr': 0.001,
               'momentum': 0.95,
               'optimizer_name': 'sgd',
               'weight_decay': 0.0001},
 'scheduler': {'T_0': 5,
               'T_mult': 2,
               'eta_min': 3e-05,
               'scheduler_name': 'cosinelr'},
 'transform': {'train': {'compose': {'transforms': [{'autoaugment': {'interpolation': 'nearest',
                                                                     'policy': 'imagenet'}},
                                                    {'totensor': {}},
                                                    {'randomflip': {'prob_hflip': 0.5,
                                                                    'prob_vflip': 0.5}},
                                                    {'colorjitter': {'contrast': 0.3}}]}},
               'val': {'totensor': {}}}}
06:21:57 01-24 [epoch 1 train] [step 1/193] [eta N/A]: lr 1.0000e-03 | loss 1.1356(1.1356)
06:21:59 01-24 [epoch 1 train] [step 50/193] [eta 0:00:04]: lr 1.0000e-03 | loss 1.0796(1.0796)
06:22:00 01-24 [epoch 1 train] [step 100/193] [eta 0:00:02]: lr 1.0000e-03 | loss 1.0926(1.1056)
06:22:02 01-24 [epoch 1 train] [step 150/193] [eta 0:00:01]: lr 1.0000e-03 | loss 1.0781(1.0491)
06:22:04 01-24 [epoch 1 train] [step 193/193] [eta 0:00:00]: lr 1.0000e-03 | loss 1.0785(1.0808)
06:22:07 01-24 [epoch 1 val]: Accuracy 48.4375 | Loss 0.9912
06:22:09 01-24 [epoch 2 train] [step 1/193] [eta 0:00:30]: lr 9.9815e-04 | loss 0.9587(1.0766)
06:22:11 01-24 [epoch 2 train] [step 50/193] [eta 0:00:19]: lr 9.0737e-04 | loss 0.9724(0.9724)
06:22:12 01-24 [epoch 2 train] [step 100/193] [eta 0:00:02]: lr 9.0737e-04 | loss 0.9612(0.9501)
06:22:14 01-24 [epoch 2 train] [step 150/193] [eta 0:00:01]: lr 9.0737e-04 | loss 0.9572(0.9492)
06:22:15 01-24 [epoch 2 train] [step 193/193] [eta 0:00:00]: lr 9.0737e-04 | loss 0.9474(0.9098)
06:22:18 01-24 [epoch 2 val]: Accuracy 46.8750 | Loss 0.8107
06:22:19 01-24 [epoch 3 train] [step 1/193] [eta 0:00:21]: lr 9.0252e-04 | loss 0.6776(0.9025)
06:22:21 01-24 [epoch 3 train] [step 50/193] [eta 0:00:15]: lr 6.6487e-04 | loss 0.8447(0.8447)
06:22:22 01-24 [epoch 3 train] [step 100/193] [eta 0:00:02]: lr 6.6487e-04 | loss 0.8260(0.8074)
06:22:23 01-24 [epoch 3 train] [step 150/193] [eta 0:00:01]: lr 6.6487e-04 | loss 0.8248(0.8223)
06:22:25 01-24 [epoch 3 train] [step 193/193] [eta 0:00:00]: lr 6.6487e-04 | loss 0.8097(0.7716)
06:22:27 01-24 [epoch 3 val]: Accuracy 64.5833 | Loss 0.6759
06:22:28 01-24 [epoch 4 train] [step 1/193] [eta 0:00:20]: lr 6.5888e-04 | loss 1.1517(0.7785)
06:22:30 01-24 [epoch 4 train] [step 50/193] [eta 0:00:15]: lr 3.6513e-04 | loss 0.7621(0.7621)
06:22:31 01-24 [epoch 4 train] [step 100/193] [eta 0:00:02]: lr 3.6513e-04 | loss 0.7377(0.7132)
06:22:33 01-24 [epoch 4 train] [step 150/193] [eta 0:00:01]: lr 3.6513e-04 | loss 0.7158(0.6721)
06:22:34 01-24 [epoch 4 train] [step 193/193] [eta 0:00:00]: lr 3.6513e-04 | loss 0.7176(0.7177)
06:22:37 01-24 [epoch 4 val]: Accuracy 67.1875 | Loss 0.6232
06:22:38 01-24 [epoch 5 train] [step 1/193] [eta 0:00:21]: lr 3.6028e-04 | loss 0.7720(0.7188)
06:22:40 01-24 [epoch 5 train] [step 50/193] [eta 0:00:18]: lr 1.2263e-04 | loss 0.6828(0.6828)
06:22:42 01-24 [epoch 5 train] [step 100/193] [eta 0:00:03]: lr 1.2263e-04 | loss 0.6868(0.6908)
06:22:45 01-24 [epoch 5 train] [step 150/193] [eta 0:00:02]: lr 1.2263e-04 | loss 0.6856(0.6833)
06:22:47 01-24 [epoch 5 train] [step 193/193] [eta 0:00:00]: lr 1.2263e-04 | loss 0.6846(0.6688)
06:22:50 01-24 [epoch 5 val]: Accuracy 68.7500 | Loss 0.6198
06:22:51 01-24 [epoch 6 train] [step 1/193] [eta 0:00:25]: lr 1.4017e-04 | loss 0.5245(0.6702)
06:22:53 01-24 [epoch 6 train] [step 50/193] [eta 0:00:17]: lr 1.0000e-03 | loss 0.6576(0.6576)
06:22:55 01-24 [epoch 6 train] [step 100/193] [eta 0:00:04]: lr 1.0000e-03 | loss 0.6554(0.6532)
06:22:57 01-24 [epoch 6 train] [step 150/193] [eta 0:00:01]: lr 1.0000e-03 | loss 0.6552(0.6548)
06:22:58 01-24 [epoch 6 train] [step 193/193] [eta 0:00:00]: lr 1.0000e-03 | loss 0.6649(0.6995)
06:23:02 01-24 [epoch 6 val]: Accuracy 71.8750 | Loss 0.5378
06:23:02 01-24 [epoch 7 train] [step 1/193] [eta 0:00:20]: lr 9.9953e-04 | loss 0.5117(0.6955)
06:23:04 01-24 [epoch 7 train] [step 50/193] [eta 0:00:15]: lr 9.7626e-04 | loss 0.5714(0.5714)
06:23:05 01-24 [epoch 7 train] [step 100/193] [eta 0:00:02]: lr 9.7626e-04 | loss 0.5951(0.6187)
06:23:07 01-24 [epoch 7 train] [step 150/193] [eta 0:00:01]: lr 9.7626e-04 | loss 0.5903(0.5809)
06:23:08 01-24 [epoch 7 train] [step 193/193] [eta 0:00:00]: lr 9.7626e-04 | loss 0.5780(0.5332)
06:23:12 01-24 [epoch 7 val]: Accuracy 69.7917 | Loss 0.5283
06:23:14 01-24 [epoch 8 train] [step 1/193] [eta 0:00:29]: lr 9.7488e-04 | loss 0.4768(0.5354)
06:23:16 01-24 [epoch 8 train] [step 50/193] [eta 0:00:22]: lr 9.0737e-04 | loss 0.5319(0.5319)
06:23:18 01-24 [epoch 8 train] [step 100/193] [eta 0:00:03]: lr 9.0737e-04 | loss 0.5335(0.5350)
06:23:19 01-24 [epoch 8 train] [step 150/193] [eta 0:00:01]: lr 9.0737e-04 | loss 0.5400(0.5532)
06:23:21 01-24 [epoch 8 train] [step 193/193] [eta 0:00:00]: lr 9.0737e-04 | loss 0.5292(0.5022)
06:23:24 01-24 [epoch 8 val]: Accuracy 67.7083 | Loss 0.5306
06:23:25 01-24 [epoch 9 train] [step 1/193] [eta 0:00:23]: lr 9.0523e-04 | loss 0.6460(0.4942)
06:23:27 01-24 [epoch 9 train] [step 50/193] [eta 0:00:17]: lr 8.0008e-04 | loss 0.4691(0.4691)
06:23:28 01-24 [epoch 9 train] [step 100/193] [eta 0:00:02]: lr 8.0008e-04 | loss 0.4676(0.4661)
06:23:30 01-24 [epoch 9 train] [step 150/193] [eta 0:00:01]: lr 8.0008e-04 | loss 0.4905(0.5365)
06:23:31 01-24 [epoch 9 train] [step 193/193] [eta 0:00:00]: lr 8.0008e-04 | loss 0.4985(0.5444)
06:23:34 01-24 [epoch 9 val]: Accuracy 69.2708 | Loss 0.5385
06:23:35 01-24 [epoch 10 train] [step 1/193] [eta 0:00:19]: lr 7.9737e-04 | loss 0.6389(0.5435)
06:23:36 01-24 [epoch 10 train] [step 50/193] [eta 0:00:14]: lr 6.6487e-04 | loss 0.4697(0.4697)
06:24:01 01-24 [epoch 10 train] [step 100/193] [eta 0:00:02]: lr 6.6487e-04 | loss 0.4760(0.4822)
06:23:39 01-24 [epoch 10 train] [step 150/193] [eta 0:00:01]: lr 6.6487e-04 | loss 0.4741(0.4703)
06:23:41 01-24 [epoch 10 train] [step 193/193] [eta 0:00:00]: lr 6.6487e-04 | loss 0.4742(0.4647)
06:23:44 01-24 [epoch 10 val]: Accuracy 69.7917 | Loss 0.5165
06:23:46 01-24 [epoch 11 train] [step 1/193] [eta 0:00:24]: lr 6.6188e-04 | loss 0.3256(0.4658)
06:24:11 01-24 [epoch 11 train] [step 50/193] [eta 0:00:19]: lr 5.1500e-04 | loss 0.4499(0.4499)
06:23:49 01-24 [epoch 11 train] [step 100/193] [eta 0:00:02]: lr 5.1500e-04 | loss 0.4524(0.4549)
06:23:51 01-24 [epoch 11 train] [step 150/193] [eta 0:00:01]: lr 5.1500e-04 | loss 0.4517(0.4504)
06:23:52 01-24 [epoch 11 train] [step 193/193] [eta 0:00:00]: lr 5.1500e-04 | loss 0.4544(0.4646)
06:23:55 01-24 [epoch 11 val]: Accuracy 69.7917 | Loss 0.4839
06:23:57 01-24 [epoch 12 train] [step 1/193] [eta 0:00:25]: lr 5.1200e-04 | loss 0.2400(0.4577)
06:23:59 01-24 [epoch 12 train] [step 50/193] [eta 0:00:19]: lr 3.6513e-04 | loss 0.3908(0.3908)
06:24:01 01-24 [epoch 12 train] [step 100/193] [eta 0:00:04]: lr 3.6513e-04 | loss 0.4013(0.4118)
06:24:03 01-24 [epoch 12 train] [step 150/193] [eta 0:00:01]: lr 3.6513e-04 | loss 0.4177(0.4505)
06:24:04 01-24 [epoch 12 train] [step 193/193] [eta 0:00:00]: lr 3.6513e-04 | loss 0.4157(0.4135)
06:24:07 01-24 [epoch 12 val]: Accuracy 71.8750 | Loss 0.4801
06:24:08 01-24 [epoch 13 train] [step 1/193] [eta 0:00:22]: lr 3.6242e-04 | loss 0.7282(0.4196)
06:24:10 01-24 [epoch 13 train] [step 50/193] [eta 0:00:16]: lr 2.2992e-04 | loss 0.4215(0.4215)
06:24:11 01-24 [epoch 13 train] [step 100/193] [eta 0:00:02]: lr 2.2992e-04 | loss 0.4344(0.4472)
06:24:36 01-24 [epoch 13 train] [step 150/193] [eta 0:00:01]: lr 2.2992e-04 | loss 0.4094(0.3596)
06:24:14 01-24 [epoch 13 train] [step 193/193] [eta 0:00:00]: lr 2.2992e-04 | loss 0.4139(0.4147)
06:24:17 01-24 [epoch 13 val]: Accuracy 70.3125 | Loss 0.4719
06:24:19 01-24 [epoch 14 train] [step 1/193] [eta 0:00:25]: lr 2.2778e-04 | loss 0.3509(0.4125)
06:24:21 01-24 [epoch 14 train] [step 50/193] [eta 0:00:20]: lr 1.2263e-04 | loss 0.3366(0.3366)
06:24:46 01-24 [epoch 14 train] [step 100/193] [eta 0:00:02]: lr 1.2263e-04 | loss 0.3741(0.4115)
06:24:24 01-24 [epoch 14 train] [step 150/193] [eta 0:00:01]: lr 1.2263e-04 | loss 0.3769(0.3824)
06:24:26 01-24 [epoch 14 train] [step 193/193] [eta 0:00:00]: lr 1.2263e-04 | loss 0.3838(0.3968)
06:24:29 01-24 [epoch 14 val]: Accuracy 73.4375 | Loss 0.4572
06:24:30 01-24 [epoch 15 train] [step 1/193] [eta 0:00:20]: lr 1.2125e-04 | loss 0.6456(0.4022)
06:24:31 01-24 [epoch 15 train] [step 50/193] [eta 0:00:15]: lr 5.3738e-05 | loss 0.3916(0.3916)
06:24:56 01-24 [epoch 15 train] [step 100/193] [eta 0:00:02]: lr 5.3738e-05 | loss 0.3822(0.3728)
06:24:35 01-24 [epoch 15 train] [step 150/193] [eta 0:00:02]: lr 5.3738e-05 | loss 0.3855(0.3919)
06:24:37 01-24 [epoch 15 train] [step 193/193] [eta 0:00:00]: lr 5.3738e-05 | loss 0.3910(0.4221)
06:24:41 01-24 [epoch 15 val]: Accuracy 73.9583 | Loss 0.4471
06:24:42 01-24 [epoch 16 train] [step 1/193] [eta 0:00:27]: lr 7.2663e-05 | loss 0.5097(0.4228)
06:24:43 01-24 [epoch 16 train] [step 50/193] [eta 0:00:17]: lr 1.0000e-03 | loss 0.3522(0.3522)
06:24:45 01-24 [epoch 16 train] [step 100/193] [eta 0:00:02]: lr 1.0000e-03 | loss 0.3848(0.4174)
06:24:47 01-24 [epoch 16 train] [step 150/193] [eta 0:00:01]: lr 1.0000e-03 | loss 0.3788(0.3667)
06:24:48 01-24 [epoch 16 train] [step 193/193] [eta 0:00:00]: lr 1.0000e-03 | loss 0.3777(0.3695)
06:24:51 01-24 [epoch 16 val]: Accuracy 65.1042 | Loss 0.5406
06:24:52 01-24 [epoch 17 train] [step 1/193] [eta 0:00:21]: lr 9.9988e-04 | loss 0.3931(0.3730)
06:24:54 01-24 [epoch 17 train] [step 50/193] [eta 0:00:17]: lr 9.9403e-04 | loss 0.4251(0.4251)
06:24:56 01-24 [epoch 17 train] [step 100/193] [eta 0:00:03]: lr 9.9403e-04 | loss 0.3861(0.3472)
06:24:57 01-24 [epoch 17 train] [step 150/193] [eta 0:00:01]: lr 9.9403e-04 | loss 0.3758(0.3553)
06:24:58 01-24 [epoch 17 train] [step 193/193] [eta 0:00:00]: lr 9.9403e-04 | loss 0.3757(0.3769)
06:25:01 01-24 [epoch 17 val]: Accuracy 72.9167 | Loss 0.4627
06:25:02 01-24 [epoch 18 train] [step 1/193] [eta 0:00:20]: lr 9.9367e-04 | loss 0.5086(0.3746)
06:25:04 01-24 [epoch 18 train] [step 50/193] [eta 0:00:15]: lr 9.7626e-04 | loss 0.3519(0.3519)
06:25:05 01-24 [epoch 18 train] [step 100/193] [eta 0:00:03]: lr 9.7626e-04 | loss 0.3677(0.3836)
06:25:07 01-24 [epoch 18 train] [step 150/193] [eta 0:00:01]: lr 9.7626e-04 | loss 0.3563(0.3335)
06:25:08 01-24 [epoch 18 train] [step 193/193] [eta 0:00:00]: lr 9.7626e-04 | loss 0.3679(0.3864)
06:25:12 01-24 [epoch 18 val]: Accuracy 70.3125 | Loss 0.5221
06:25:15 01-24 [epoch 19 train] [step 1/193] [eta 0:00:29]: lr 9.7568e-04 | loss 0.2506(0.3841)
06:25:17 01-24 [epoch 19 train] [step 50/193] [eta 0:00:24]: lr 9.4714e-04 | loss 0.3485(0.3485)
06:25:18 01-24 [epoch 19 train] [step 100/193] [eta 0:00:02]: lr 9.4714e-04 | loss 0.3594(0.3703)
06:25:20 01-24 [epoch 19 train] [step 150/193] [eta 0:00:01]: lr 9.4714e-04 | loss 0.3416(0.3059)
06:25:21 01-24 [epoch 19 train] [step 193/193] [eta 0:00:00]: lr 9.4714e-04 | loss 0.3380(0.3275)
06:25:24 01-24 [epoch 19 val]: Accuracy 76.0417 | Loss 0.4067
06:25:25 01-24 [epoch 20 train] [step 1/193] [eta 0:00:22]: lr 9.4634e-04 | loss 0.6168(0.3346)
06:25:28 01-24 [epoch 20 train] [step 50/193] [eta 0:00:19]: lr 9.0737e-04 | loss 0.3330(0.3330)
06:25:30 01-24 [epoch 20 train] [step 100/193] [eta 0:00:03]: lr 9.0737e-04 | loss 0.3081(0.2832)
06:25:32 01-24 [epoch 20 train] [step 150/193] [eta 0:00:01]: lr 9.0737e-04 | loss 0.3173(0.3356)
06:25:33 01-24 [epoch 20 train] [step 193/193] [eta 0:00:00]: lr 9.0737e-04 | loss 0.3176(0.3317)
06:25:37 01-24 [epoch 20 val]: Accuracy 74.4792 | Loss 0.4450
06:25:38 01-24 [epoch 21 train] [step 1/193] [eta 0:00:23]: lr 9.0638e-04 | loss 0.1783(0.3302)
06:25:39 01-24 [epoch 21 train] [step 50/193] [eta 0:00:16]: lr 8.5795e-04 | loss 0.2946(0.2946)
06:25:41 01-24 [epoch 21 train] [step 100/193] [eta 0:00:03]: lr 8.5795e-04 | loss 0.2934(0.2922)
06:25:42 01-24 [epoch 21 train] [step 150/193] [eta 0:00:01]: lr 8.5795e-04 | loss 0.2925(0.2907)
06:25:43 01-24 [epoch 21 train] [step 193/193] [eta 0:00:00]: lr 8.5795e-04 | loss 0.2875(0.2657)
06:25:46 01-24 [epoch 21 val]: Accuracy 71.8750 | Loss 0.5023
06:25:48 01-24 [epoch 22 train] [step 1/193] [eta 0:00:21]: lr 8.5679e-04 | loss 0.7383(0.2763)
06:25:50 01-24 [epoch 22 train] [step 50/193] [eta 0:00:18]: lr 8.0008e-04 | loss 0.2832(0.2832)
06:25:52 01-24 [epoch 22 train] [step 100/193] [eta 0:00:04]: lr 8.0008e-04 | loss 0.2768(0.2705)
06:25:55 01-24 [epoch 22 train] [step 150/193] [eta 0:00:02]: lr 8.0008e-04 | loss 0.2676(0.2491)
06:25:57 01-24 [epoch 22 train] [step 193/193] [eta 0:00:00]: lr 8.0008e-04 | loss 0.2705(0.2746)
06:26:00 01-24 [epoch 22 val]: Accuracy 72.3958 | Loss 0.6044
06:26:03 01-24 [epoch 23 train] [step 1/193] [eta 0:00:31]: lr 7.9878e-04 | loss 0.2310(0.2748)
06:26:05 01-24 [epoch 23 train] [step 50/193] [eta 0:00:23]: lr 7.3519e-04 | loss 0.2440(0.2440)
06:26:06 01-24 [epoch 23 train] [step 100/193] [eta 0:00:03]: lr 7.3519e-04 | loss 0.2587(0.2734)
06:26:31 01-24 [epoch 23 train] [step 150/193] [eta 0:00:01]: lr 7.3519e-04 | loss 0.2607(0.2647)
06:26:09 01-24 [epoch 23 train] [step 193/193] [eta 0:00:00]: lr 7.3519e-04 | loss 0.2701(0.2947)
06:26:13 01-24 [epoch 23 val]: Accuracy 73.9583 | Loss 0.5047
06:26:13 01-24 [epoch 24 train] [step 1/193] [eta 0:00:21]: lr 7.3378e-04 | loss 0.1618(0.2916)
06:26:15 01-24 [epoch 24 train] [step 50/193] [eta 0:00:15]: lr 6.6487e-04 | loss 0.2672(0.2672)
06:26:16 01-24 [epoch 24 train] [step 100/193] [eta 0:00:02]: lr 6.6487e-04 | loss 0.2593(0.2513)
06:26:18 01-24 [epoch 24 train] [step 150/193] [eta 0:00:01]: lr 6.6487e-04 | loss 0.2532(0.2409)
06:26:19 01-24 [epoch 24 train] [step 193/193] [eta 0:00:00]: lr 6.6487e-04 | loss 0.2493(0.2479)
06:26:22 01-24 [epoch 24 val]: Accuracy 72.3958 | Loss 0.5147
06:26:46 01-24 [epoch 25 train] [step 1/193] [eta 0:00:21]: lr 6.6339e-04 | loss 0.4911(0.2542)
06:26:25 01-24 [epoch 25 train] [step 50/193] [eta 0:00:16]: lr 5.9087e-04 | loss 0.2754(0.2754)
06:26:26 01-24 [epoch 25 train] [step 100/193] [eta 0:00:03]: lr 5.9087e-04 | loss 0.2711(0.2669)
06:26:29 01-24 [epoch 25 train] [step 150/193] [eta 0:00:02]: lr 5.9087e-04 | loss 0.2628(0.2460)
06:26:32 01-24 [epoch 25 train] [step 193/193] [eta 0:00:00]: lr 5.9087e-04 | loss 0.2542(0.2246)
06:26:36 01-24 [epoch 25 val]: Accuracy 75.0000 | Loss 0.5036
06:26:36 01-24 [epoch 26 train] [step 1/193] [eta 0:00:27]: lr 5.8935e-04 | loss 0.3047(0.2274)
06:26:38 01-24 [epoch 26 train] [step 50/193] [eta 0:00:16]: lr 5.1500e-04 | loss 0.2148(0.2148)
06:26:39 01-24 [epoch 26 train] [step 100/193] [eta 0:00:02]: lr 5.1500e-04 | loss 0.2242(0.2335)
06:26:41 01-24 [epoch 26 train] [step 150/193] [eta 0:00:01]: lr 5.1500e-04 | loss 0.2376(0.2644)
06:26:42 01-24 [epoch 26 train] [step 193/193] [eta 0:00:00]: lr 5.1500e-04 | loss 0.2339(0.2269)
06:26:46 01-24 [epoch 26 val]: Accuracy 74.4792 | Loss 0.5701
06:26:47 01-24 [epoch 27 train] [step 1/193] [eta 0:00:22]: lr 5.1348e-04 | loss 0.1665(0.2248)
06:26:48 01-24 [epoch 27 train] [step 50/193] [eta 0:00:16]: lr 4.3913e-04 | loss 0.2204(0.2204)
06:26:50 01-24 [epoch 27 train] [step 100/193] [eta 0:00:03]: lr 4.3913e-04 | loss 0.2268(0.2332)
06:26:51 01-24 [epoch 27 train] [step 150/193] [eta 0:00:01]: lr 4.3913e-04 | loss 0.2250(0.2213)
06:26:53 01-24 [epoch 27 train] [step 193/193] [eta 0:00:00]: lr 4.3913e-04 | loss 0.2258(0.2413)
06:26:56 01-24 [epoch 27 val]: Accuracy 73.9583 | Loss 0.5799
06:26:57 01-24 [epoch 28 train] [step 1/193] [eta 0:00:24]: lr 4.3765e-04 | loss 0.4699(0.2471)
06:26:59 01-24 [epoch 28 train] [step 50/193] [eta 0:00:18]: lr 3.6513e-04 | loss 0.2015(0.2015)
06:27:01 01-24 [epoch 28 train] [step 100/193] [eta 0:00:02]: lr 3.6513e-04 | loss 0.2149(0.2282)
06:27:03 01-24 [epoch 28 train] [step 150/193] [eta 0:00:01]: lr 3.6513e-04 | loss 0.2171(0.2215)
06:27:04 01-24 [epoch 28 train] [step 193/193] [eta 0:00:00]: lr 3.6513e-04 | loss 0.2238(0.2381)
06:27:09 01-24 [epoch 28 val]: Accuracy 76.5625 | Loss 0.4605
06:27:11 01-24 [epoch 29 train] [step 1/193] [eta 0:00:32]: lr 3.6372e-04 | loss 0.1547(0.2376)
06:27:12 01-24 [epoch 29 train] [step 50/193] [eta 0:00:23]: lr 2.9481e-04 | loss 0.2095(0.2095)
06:27:14 01-24 [epoch 29 train] [step 100/193] [eta 0:00:03]: lr 2.9481e-04 | loss 0.2075(0.2054)
06:27:16 01-24 [epoch 29 train] [step 150/193] [eta 0:00:01]: lr 2.9481e-04 | loss 0.2086(0.2109)
06:27:17 01-24 [epoch 29 train] [step 193/193] [eta 0:00:00]: lr 2.9481e-04 | loss 0.2099(0.2126)
06:27:20 01-24 [epoch 29 val]: Accuracy 74.4792 | Loss 0.4999
06:27:22 01-24 [epoch 30 train] [step 1/193] [eta 0:00:22]: lr 2.9352e-04 | loss 0.1976(0.2134)
06:27:23 01-24 [epoch 30 train] [step 50/193] [eta 0:00:16]: lr 2.2992e-04 | loss 0.1981(0.1981)
06:27:24 01-24 [epoch 30 train] [step 100/193] [eta 0:00:03]: lr 2.2992e-04 | loss 0.1984(0.1986)
06:27:26 01-24 [epoch 30 train] [step 150/193] [eta 0:00:01]: lr 2.2992e-04 | loss 0.2033(0.2132)
06:27:28 01-24 [epoch 30 train] [step 193/193] [eta 0:00:00]: lr 2.2992e-04 | loss 0.2122(0.2464)
06:27:31 01-24 [epoch 30 val]: Accuracy 73.9583 | Loss 0.5784
06:27:31 01-24 [epoch 31 train] [step 1/193] [eta 0:00:20]: lr 2.2877e-04 | loss 0.1694(0.2463)
06:27:33 01-24 [epoch 31 train] [step 50/193] [eta 0:00:15]: lr 1.7205e-04 | loss 0.2048(0.2048)
06:27:34 01-24 [epoch 31 train] [step 100/193] [eta 0:00:02]: lr 1.7205e-04 | loss 0.2068(0.2088)
06:27:36 01-24 [epoch 31 train] [step 150/193] [eta 0:00:01]: lr 1.7205e-04 | loss 0.2103(0.2172)
06:27:37 01-24 [epoch 31 train] [step 193/193] [eta 0:00:00]: lr 1.7205e-04 | loss 0.2147(0.2418)
06:27:41 01-24 [epoch 31 val]: Accuracy 73.9583 | Loss 0.5498
06:27:44 01-24 [epoch 32 train] [step 1/193] [eta 0:00:30]: lr 1.7106e-04 | loss 0.1626(0.2278)
06:27:46 01-24 [epoch 32 train] [step 50/193] [eta 0:00:25]: lr 1.2263e-04 | loss 0.2250(0.2250)
06:28:11 01-24 [epoch 32 train] [step 100/193] [eta 0:00:04]: lr 1.2263e-04 | loss 0.2169(0.2088)
06:27:50 01-24 [epoch 32 train] [step 150/193] [eta 0:00:01]: lr 1.2263e-04 | loss 0.2107(0.1985)
06:27:51 01-24 [epoch 32 train] [step 193/193] [eta 0:00:00]: lr 1.2263e-04 | loss 0.2112(0.2091)
06:27:54 01-24 [epoch 32 val]: Accuracy 75.0000 | Loss 0.4992
06:27:55 01-24 [epoch 33 train] [step 1/193] [eta 0:00:22]: lr 1.2183e-04 | loss 0.1760(0.2094)
06:27:57 01-24 [epoch 33 train] [step 50/193] [eta 0:00:17]: lr 8.2862e-05 | loss 0.1945(0.1945)
06:27:59 01-24 [epoch 33 train] [step 100/193] [eta 0:00:03]: lr 8.2862e-05 | loss 0.2022(0.2099)
06:28:00 01-24 [epoch 33 train] [step 150/193] [eta 0:00:01]: lr 8.2862e-05 | loss 0.2119(0.2313)
06:28:02 01-24 [epoch 33 train] [step 193/193] [eta 0:00:00]: lr 8.2862e-05 | loss 0.2036(0.1761)
06:28:05 01-24 [epoch 33 val]: Accuracy 75.5208 | Loss 0.5131
06:28:06 01-24 [epoch 34 train] [step 1/193] [eta 0:00:21]: lr 8.2279e-05 | loss 0.1655(0.1741)
06:28:07 01-24 [epoch 34 train] [step 50/193] [eta 0:00:15]: lr 5.3738e-05 | loss 0.2091(0.2091)
06:28:09 01-24 [epoch 34 train] [step 100/193] [eta 0:00:02]: lr 5.3738e-05 | loss 0.2212(0.2332)
06:28:10 01-24 [epoch 34 train] [step 150/193] [eta 0:00:01]: lr 5.3738e-05 | loss 0.2114(0.1920)
06:28:11 01-24 [epoch 34 train] [step 193/193] [eta 0:00:00]: lr 5.3738e-05 | loss 0.2054(0.1839)
06:28:16 01-24 [epoch 34 val]: Accuracy 75.0000 | Loss 0.5295
06:28:17 01-24 [epoch 35 train] [step 1/193] [eta 0:00:28]: lr 5.3382e-05 | loss 0.1623(0.1841)
06:28:20 01-24 [epoch 35 train] [step 50/193] [eta 0:00:24]: lr 3.5971e-05 | loss 0.2224(0.2224)
06:28:46 01-24 [epoch 35 train] [step 100/193] [eta 0:00:05]: lr 3.5971e-05 | loss 0.2201(0.2178)
06:28:26 01-24 [epoch 35 train] [step 150/193] [eta 0:00:02]: lr 3.5971e-05 | loss 0.2118(0.1952)
06:28:27 01-24 [epoch 35 train] [step 193/193] [eta 0:00:00]: lr 3.5971e-05 | loss 0.2080(0.2064)
06:28:30 01-24 [epoch 35 val]: Accuracy 73.4375 | Loss 0.5447
06:28:31 01-24 [epoch 36 train] [step 1/193] [eta 0:00:21]: lr 5.5252e-05 | loss 0.1591(0.2065)
06:28:33 01-24 [epoch 36 train] [step 50/193] [eta 0:00:15]: lr 1.0000e-03 | loss 0.2048(0.2048)
06:28:34 01-24 [epoch 36 train] [step 100/193] [eta 0:00:02]: lr 1.0000e-03 | loss 0.1933(0.1818)
06:28:36 01-24 [epoch 36 train] [step 150/193] [eta 0:00:01]: lr 1.0000e-03 | loss 0.2088(0.2399)
06:28:37 01-24 [epoch 36 train] [step 193/193] [eta 0:00:00]: lr 1.0000e-03 | loss 0.2170(0.2502)
06:28:41 01-24 [epoch 36 val]: Accuracy 77.6042 | Loss 0.4891
06:28:41 01-24 [epoch 37 train] [step 1/193] [eta 0:00:23]: lr 9.9997e-04 | loss 0.1649(0.2385)
06:28:43 01-24 [epoch 37 train] [step 50/193] [eta 0:00:15]: lr 9.9850e-04 | loss 0.2134(0.2134)
06:28:44 01-24 [epoch 37 train] [step 100/193] [eta 0:00:02]: lr 9.9850e-04 | loss 0.1964(0.1794)
06:28:46 01-24 [epoch 37 train] [step 150/193] [eta 0:00:01]: lr 9.9850e-04 | loss 0.2171(0.2586)
06:28:47 01-24 [epoch 37 train] [step 193/193] [eta 0:00:00]: lr 9.9850e-04 | loss 0.2230(0.2491)
06:28:51 01-24 [epoch 37 val]: Accuracy 69.7917 | Loss 0.6291
06:28:52 01-24 [epoch 38 train] [step 1/193] [eta 0:00:24]: lr 9.9842e-04 | loss 0.1613(0.2490)
06:28:54 01-24 [epoch 38 train] [step 50/193] [eta 0:00:18]: lr 9.9403e-04 | loss 0.2473(0.2473)
06:28:55 01-24 [epoch 38 train] [step 100/193] [eta 0:00:03]: lr 9.9403e-04 | loss 0.2259(0.2044)
06:28:58 01-24 [epoch 38 train] [step 150/193] [eta 0:00:02]: lr 9.9403e-04 | loss 0.2367(0.2585)
06:29:01 01-24 [epoch 38 train] [step 193/193] [eta 0:00:00]: lr 9.9403e-04 | loss 0.2246(0.1811)
06:29:04 01-24 [epoch 38 val]: Accuracy 72.3958 | Loss 0.7323
06:29:05 01-24 [epoch 39 train] [step 1/193] [eta 0:00:27]: lr 9.9388e-04 | loss 0.2057(0.1819)
06:29:07 01-24 [epoch 39 train] [step 50/193] [eta 0:00:18]: lr 9.8660e-04 | loss 0.2354(0.2354)
06:29:09 01-24 [epoch 39 train] [step 100/193] [eta 0:00:03]: lr 9.8660e-04 | loss 0.2291(0.2228)
06:29:11 01-24 [epoch 39 train] [step 150/193] [eta 0:00:01]: lr 9.8660e-04 | loss 0.2155(0.1882)
06:29:12 01-24 [epoch 39 train] [step 193/193] [eta 0:00:00]: lr 9.8660e-04 | loss 0.2174(0.2155)
06:29:15 01-24 [epoch 39 val]: Accuracy 74.4792 | Loss 0.5482
06:29:16 01-24 [epoch 40 train] [step 1/193] [eta 0:00:21]: lr 9.8639e-04 | loss 0.1544(0.2154)
06:29:18 01-24 [epoch 40 train] [step 50/193] [eta 0:00:15]: lr 9.7626e-04 | loss 0.2446(0.2446)
06:29:19 01-24 [epoch 40 train] [step 100/193] [eta 0:00:03]: lr 9.7626e-04 | loss 0.2173(0.1899)
06:29:22 01-24 [epoch 40 train] [step 150/193] [eta 0:00:01]: lr 9.7626e-04 | loss 0.2082(0.1900)
06:29:24 01-24 [epoch 40 train] [step 193/193] [eta 0:00:00]: lr 9.7626e-04 | loss 0.2068(0.2028)
06:29:28 01-24 [epoch 40 val]: Accuracy 75.5208 | Loss 0.5855
06:29:28 01-24 [epoch 41 train] [step 1/193] [eta 0:00:27]: lr 9.7600e-04 | loss 0.1588(0.2029)
06:29:30 01-24 [epoch 41 train] [step 50/193] [eta 0:00:15]: lr 9.6308e-04 | loss 0.2072(0.2072)
06:29:32 01-24 [epoch 41 train] [step 100/193] [eta 0:00:03]: lr 9.6308e-04 | loss 0.2024(0.1975)
06:29:33 01-24 [epoch 41 train] [step 150/193] [eta 0:00:01]: lr 9.6308e-04 | loss 0.1949(0.1801)
06:29:35 01-24 [epoch 41 train] [step 193/193] [eta 0:00:00]: lr 9.6308e-04 | loss 0.1922(0.1806)
06:29:40 01-24 [epoch 41 val]: Accuracy 73.4375 | Loss 0.6085
06:29:41 01-24 [epoch 42 train] [step 1/193] [eta 0:00:32]: lr 9.6276e-04 | loss 0.1548(0.1806)
06:29:43 01-24 [epoch 42 train] [step 50/193] [eta 0:00:21]: lr 9.4714e-04 | loss 0.2002(0.2002)
06:29:44 01-24 [epoch 42 train] [step 100/193] [eta 0:00:02]: lr 9.4714e-04 | loss 0.2095(0.2188)
06:29:46 01-24 [epoch 42 train] [step 150/193] [eta 0:00:01]: lr 9.4714e-04 | loss 0.2081(0.2054)
06:29:47 01-24 [epoch 42 train] [step 193/193] [eta 0:00:00]: lr 9.4714e-04 | loss 0.2083(0.2057)
06:29:50 01-24 [epoch 42 val]: Accuracy 76.0417 | Loss 0.5913
06:29:51 01-24 [epoch 43 train] [step 1/193] [eta 0:00:21]: lr 9.4677e-04 | loss 0.1555(0.2057)
06:29:53 01-24 [epoch 43 train] [step 50/193] [eta 0:00:15]: lr 9.2853e-04 | loss 0.1812(0.1812)
06:29:55 01-24 [epoch 43 train] [step 100/193] [eta 0:00:04]: lr 9.2853e-04 | loss 0.1823(0.1835)
06:29:58 01-24 [epoch 43 train] [step 150/193] [eta 0:00:02]: lr 9.2853e-04 | loss 0.1874(0.1976)
06:29:59 01-24 [epoch 43 train] [step 193/193] [eta 0:00:00]: lr 9.2853e-04 | loss 0.1863(0.1793)
06:30:02 01-24 [epoch 43 val]: Accuracy 72.3958 | Loss 0.7100
06:30:26 01-24 [epoch 44 train] [step 1/193] [eta 0:00:22]: lr 9.2811e-04 | loss 0.1624(0.1794)
06:30:05 01-24 [epoch 44 train] [step 50/193] [eta 0:00:16]: lr 9.0737e-04 | loss 0.1807(0.1807)
06:30:06 01-24 [epoch 44 train] [step 100/193] [eta 0:00:02]: lr 9.0737e-04 | loss 0.1811(0.1815)
06:30:08 01-24 [epoch 44 train] [step 150/193] [eta 0:00:01]: lr 9.0737e-04 | loss 0.1882(0.2024)
06:30:09 01-24 [epoch 44 train] [step 193/193] [eta 0:00:00]: lr 9.0737e-04 | loss 0.1894(0.1920)
06:30:36 01-24 [epoch 44 val]: Accuracy 77.0833 | Loss 0.7873
06:30:16 01-24 [epoch 45 train] [step 1/193] [eta 0:00:31]: lr 9.0690e-04 | loss 0.1586(0.1897)
06:30:18 01-24 [epoch 45 train] [step 50/193] [eta 0:00:25]: lr 8.8380e-04 | loss 0.2103(0.2103)
06:30:19 01-24 [epoch 45 train] [step 100/193] [eta 0:00:03]: lr 8.8380e-04 | loss 0.1896(0.1689)
06:30:21 01-24 [epoch 45 train] [step 150/193] [eta 0:00:01]: lr 8.8380e-04 | loss 0.1821(0.1671)
06:30:22 01-24 [epoch 45 train] [step 193/193] [eta 0:00:00]: lr 8.8380e-04 | loss 0.1808(0.1734)
06:30:26 01-24 [epoch 45 val]: Accuracy 75.0000 | Loss 0.6527
06:30:27 01-24 [epoch 46 train] [step 1/193] [eta 0:00:23]: lr 8.8328e-04 | loss 0.1579(0.1733)
06:30:51 01-24 [epoch 46 train] [step 50/193] [eta 0:00:17]: lr 8.5795e-04 | loss 0.2034(0.2034)
06:30:31 01-24 [epoch 46 train] [step 100/193] [eta 0:00:04]: lr 8.5795e-04 | loss 0.1922(0.1809)
06:30:32 01-24 [epoch 46 train] [step 150/193] [eta 0:00:01]: lr 8.5795e-04 | loss 0.1951(0.2009)
06:30:34 01-24 [epoch 46 train] [step 193/193] [eta 0:00:00]: lr 8.5795e-04 | loss 0.1962(0.1944)
06:30:37 01-24 [epoch 46 val]: Accuracy 75.5208 | Loss 0.6617
06:30:38 01-24 [epoch 47 train] [step 1/193] [eta 0:00:21]: lr 8.5739e-04 | loss 0.1596(0.1945)
06:30:39 01-24 [epoch 47 train] [step 50/193] [eta 0:00:16]: lr 8.2998e-04 | loss 0.1789(0.1789)
06:30:41 01-24 [epoch 47 train] [step 100/193] [eta 0:00:02]: lr 8.2998e-04 | loss 0.1725(0.1661)
06:30:42 01-24 [epoch 47 train] [step 150/193] [eta 0:00:01]: lr 8.2998e-04 | loss 0.1719(0.1709)
06:30:44 01-24 [epoch 47 train] [step 193/193] [eta 0:00:00]: lr 8.2998e-04 | loss 0.1957(0.2611)
06:30:47 01-24 [epoch 47 val]: Accuracy 76.0417 | Loss 0.6294
06:30:48 01-24 [epoch 48 train] [step 1/193] [eta 0:00:22]: lr 8.2938e-04 | loss 0.1577(0.2612)
06:30:51 01-24 [epoch 48 train] [step 50/193] [eta 0:00:18]: lr 8.0008e-04 | loss 0.1634(0.1634)
06:30:53 01-24 [epoch 48 train] [step 100/193] [eta 0:00:03]: lr 8.0008e-04 | loss 0.1690(0.1745)
06:30:55 01-24 [epoch 48 train] [step 150/193] [eta 0:00:01]: lr 8.0008e-04 | loss 0.1728(0.1805)
06:30:57 01-24 [epoch 48 train] [step 193/193] [eta 0:00:00]: lr 8.0008e-04 | loss 0.1699(0.1589)
06:31:00 01-24 [epoch 48 val]: Accuracy 70.8333 | Loss 1.1098
06:31:01 01-24 [epoch 49 train] [step 1/193] [eta 0:00:24]: lr 7.9944e-04 | loss 0.6757(0.1693)
06:31:26 01-24 [epoch 49 train] [step 50/193] [eta 0:00:18]: lr 7.6841e-04 | loss 0.1729(0.1729)
06:31:05 01-24 [epoch 49 train] [step 100/193] [eta 0:00:03]: lr 7.6841e-04 | loss 0.1755(0.1781)
06:31:07 01-24 [epoch 49 train] [step 150/193] [eta 0:00:01]: lr 7.6841e-04 | loss 0.1802(0.1897)
06:31:31 01-24 [epoch 49 train] [step 193/193] [eta 0:00:00]: lr 7.6841e-04 | loss 0.1779(0.1677)
06:31:11 01-24 [epoch 49 val]: Accuracy 71.3542 | Loss 0.9083
06:31:12 01-24 [epoch 50 train] [step 1/193] [eta 0:00:20]: lr 7.6775e-04 | loss 0.1541(0.1676)
06:31:14 01-24 [epoch 50 train] [step 50/193] [eta 0:00:15]: lr 7.3519e-04 | loss 0.1820(0.1820)
06:31:16 01-24 [epoch 50 train] [step 100/193] [eta 0:00:03]: lr 7.3519e-04 | loss 0.1823(0.1827)
06:31:17 01-24 [epoch 50 train] [step 150/193] [eta 0:00:01]: lr 7.3519e-04 | loss 0.1896(0.2041)
06:31:41 01-24 [epoch 50 train] [step 193/193] [eta 0:00:00]: lr 7.3519e-04 | loss 0.1857(0.1811)
06:31:22 01-24 [epoch 50 val]: Accuracy 73.9583 | Loss 0.8063
06:31:22 01-24 [epoch 51 train] [step 1/193] [eta 0:00:21]: lr 7.3449e-04 | loss 0.1534(0.1800)
06:31:24 01-24 [epoch 51 train] [step 50/193] [eta 0:00:16]: lr 7.0060e-04 | loss 0.2047(0.2047)
06:31:25 01-24 [epoch 51 train] [step 100/193] [eta 0:00:02]: lr 7.0060e-04 | loss 0.1880(0.1712)
06:31:27 01-24 [epoch 51 train] [step 150/193] [eta 0:00:01]: lr 7.0060e-04 | loss 0.1837(0.1750)
06:31:29 01-24 [epoch 51 train] [step 193/193] [eta 0:00:00]: lr 7.0060e-04 | loss 0.1834(0.1866)
06:31:33 01-24 [epoch 51 val]: Accuracy 71.8750 | Loss 0.8473
06:31:34 01-24 [epoch 52 train] [step 1/193] [eta 0:00:25]: lr 6.9989e-04 | loss 0.1632(0.1868)
06:31:35 01-24 [epoch 52 train] [step 50/193] [eta 0:00:16]: lr 6.6487e-04 | loss 0.1947(0.1947)
06:31:38 01-24 [epoch 52 train] [step 100/193] [eta 0:00:05]: lr 6.6487e-04 | loss 0.2005(0.2063)
06:31:39 01-24 [epoch 52 train] [step 150/193] [eta 0:00:01]: lr 6.6487e-04 | loss 0.1918(0.1744)
06:31:41 01-24 [epoch 52 train] [step 193/193] [eta 0:00:00]: lr 6.6487e-04 | loss 0.1971(0.2075)
06:31:44 01-24 [epoch 52 val]: Accuracy 77.0833 | Loss 0.6407
06:31:45 01-24 [epoch 53 train] [step 1/193] [eta 0:00:20]: lr 6.6414e-04 | loss 0.1537(0.2074)
06:31:46 01-24 [epoch 53 train] [step 50/193] [eta 0:00:14]: lr 6.2822e-04 | loss 0.1632(0.1632)
06:31:47 01-24 [epoch 53 train] [step 100/193] [eta 0:00:02]: lr 6.2822e-04 | loss 0.1832(0.2031)
06:31:49 01-24 [epoch 53 train] [step 150/193] [eta 0:00:01]: lr 6.2822e-04 | loss 0.1803(0.1745)
06:31:51 01-24 [epoch 53 train] [step 193/193] [eta 0:00:00]: lr 6.2822e-04 | loss 0.1764(0.1621)
06:31:54 01-24 [epoch 53 val]: Accuracy 78.1250 | Loss 0.6133
06:31:55 01-24 [epoch 54 train] [step 1/193] [eta 0:00:22]: lr 6.2747e-04 | loss 0.1539(0.1621)
06:31:56 01-24 [epoch 54 train] [step 50/193] [eta 0:00:16]: lr 5.9087e-04 | loss 0.1890(0.1890)
06:31:58 01-24 [epoch 54 train] [step 100/193] [eta 0:00:03]: lr 5.9087e-04 | loss 0.1775(0.1660)
06:32:00 01-24 [epoch 54 train] [step 150/193] [eta 0:00:01]: lr 5.9087e-04 | loss 0.1722(0.1617)
06:32:01 01-24 [epoch 54 train] [step 193/193] [eta 0:00:00]: lr 5.9087e-04 | loss 0.1778(0.1919)
06:32:04 01-24 [epoch 54 val]: Accuracy 75.5208 | Loss 0.6165
06:32:07 01-24 [epoch 55 train] [step 1/193] [eta 0:00:29]: lr 5.9011e-04 | loss 0.1790(0.1924)
06:32:09 01-24 [epoch 55 train] [step 50/193] [eta 0:00:24]: lr 5.5305e-04 | loss 0.1852(0.1852)
06:32:12 01-24 [epoch 55 train] [step 100/193] [eta 0:00:04]: lr 5.5305e-04 | loss 0.1728(0.1605)
06:32:14 01-24 [epoch 55 train] [step 150/193] [eta 0:00:01]: lr 5.5305e-04 | loss 0.1777(0.1875)
06:32:15 01-24 [epoch 55 train] [step 193/193] [eta 0:00:00]: lr 5.5305e-04 | loss 0.1762(0.1695)
06:32:19 01-24 [epoch 55 val]: Accuracy 78.1250 | Loss 0.5454
06:32:20 01-24 [epoch 56 train] [step 1/193] [eta 0:00:23]: lr 5.5229e-04 | loss 0.1602(0.1688)
06:32:21 01-24 [epoch 56 train] [step 50/193] [eta 0:00:16]: lr 5.1500e-04 | loss 0.1577(0.1577)
06:32:22 01-24 [epoch 56 train] [step 100/193] [eta 0:00:02]: lr 5.1500e-04 | loss 0.1651(0.1725)
06:32:24 01-24 [epoch 56 train] [step 150/193] [eta 0:00:01]: lr 5.1500e-04 | loss 0.1727(0.1880)
06:32:25 01-24 [epoch 56 train] [step 193/193] [eta 0:00:00]: lr 5.1500e-04 | loss 0.1717(0.1673)
06:32:28 01-24 [epoch 56 val]: Accuracy 75.5208 | Loss 0.6175
06:32:29 01-24 [epoch 57 train] [step 1/193] [eta 0:00:19]: lr 5.1424e-04 | loss 0.1536(0.1672)
06:32:30 01-24 [epoch 57 train] [step 50/193] [eta 0:00:15]: lr 4.7695e-04 | loss 0.1643(0.1643)
06:32:32 01-24 [epoch 57 train] [step 100/193] [eta 0:00:02]: lr 4.7695e-04 | loss 0.1622(0.1601)
06:32:56 01-24 [epoch 57 train] [step 150/193] [eta 0:00:01]: lr 4.7695e-04 | loss 0.1628(0.1641)
06:32:35 01-24 [epoch 57 train] [step 193/193] [eta 0:00:00]: lr 4.7695e-04 | loss 0.1634(0.1639)
06:32:38 01-24 [epoch 57 val]: Accuracy 71.8750 | Loss 0.7737
06:32:39 01-24 [epoch 58 train] [step 1/193] [eta 0:00:21]: lr 4.7619e-04 | loss 0.1535(0.1639)
06:32:41 01-24 [epoch 58 train] [step 50/193] [eta 0:00:17]: lr 4.3913e-04 | loss 0.1756(0.1756)
06:32:43 01-24 [epoch 58 train] [step 100/193] [eta 0:00:03]: lr 4.3913e-04 | loss 0.1743(0.1729)
06:32:45 01-24 [epoch 58 train] [step 150/193] [eta 0:00:02]: lr 4.3913e-04 | loss 0.1709(0.1642)
06:32:47 01-24 [epoch 58 train] [step 193/193] [eta 0:00:00]: lr 4.3913e-04 | loss 0.1720(0.1728)
06:32:50 01-24 [epoch 58 val]: Accuracy 73.9583 | Loss 0.7696
06:32:51 01-24 [epoch 59 train] [step 1/193] [eta 0:00:22]: lr 4.3838e-04 | loss 0.1549(0.1728)
06:32:52 01-24 [epoch 59 train] [step 50/193] [eta 0:00:15]: lr 4.0178e-04 | loss 0.1680(0.1680)
06:32:54 01-24 [epoch 59 train] [step 100/193] [eta 0:00:02]: lr 4.0178e-04 | loss 0.1623(0.1565)
06:32:56 01-24 [epoch 59 train] [step 150/193] [eta 0:00:01]: lr 4.0178e-04 | loss 0.1633(0.1653)
06:32:57 01-24 [epoch 59 train] [step 193/193] [eta 0:00:00]: lr 4.0178e-04 | loss 0.1641(0.1656)
06:33:00 01-24 [epoch 59 val]: Accuracy 74.4792 | Loss 0.7198
06:33:01 01-24 [epoch 60 train] [step 1/193] [eta 0:00:21]: lr 4.0105e-04 | loss 0.1538(0.1656)
06:33:03 01-24 [epoch 60 train] [step 50/193] [eta 0:00:16]: lr 3.6513e-04 | loss 0.1559(0.1559)
06:33:04 01-24 [epoch 60 train] [step 100/193] [eta 0:00:02]: lr 3.6513e-04 | loss 0.1829(0.2099)
06:33:06 01-24 [epoch 60 train] [step 150/193] [eta 0:00:01]: lr 3.6513e-04 | loss 0.1764(0.1632)
06:33:07 01-24 [epoch 60 train] [step 193/193] [eta 0:00:00]: lr 3.6513e-04 | loss 0.1748(0.1676)
06:33:10 01-24 [epoch 60 val]: Accuracy 75.0000 | Loss 0.6721
06:33:11 01-24 [epoch 61 train] [step 1/193] [eta 0:00:20]: lr 3.6441e-04 | loss 0.1675(0.1677)
06:33:13 01-24 [epoch 61 train] [step 50/193] [eta 0:00:16]: lr 3.2940e-04 | loss 0.2104(0.2104)
06:33:14 01-24 [epoch 61 train] [step 100/193] [eta 0:00:02]: lr 3.2940e-04 | loss 0.1851(0.1597)
06:33:16 01-24 [epoch 61 train] [step 150/193] [eta 0:00:01]: lr 3.2940e-04 | loss 0.1765(0.1594)
06:33:18 01-24 [epoch 61 train] [step 193/193] [eta 0:00:00]: lr 3.2940e-04 | loss 0.1724(0.1589)
06:33:23 01-24 [epoch 61 val]: Accuracy 79.1667 | Loss 0.6094
06:33:26 01-24 [epoch 62 train] [step 1/193] [eta 0:00:38]: lr 3.2871e-04 | loss 0.1534(0.1589)
06:33:27 01-24 [epoch 62 train] [step 50/193] [eta 0:00:25]: lr 2.9481e-04 | loss 0.1575(0.1575)
06:33:29 01-24 [epoch 62 train] [step 100/193] [eta 0:00:02]: lr 2.9481e-04 | loss 0.1580(0.1584)
06:33:30 01-24 [epoch 62 train] [step 150/193] [eta 0:00:01]: lr 2.9481e-04 | loss 0.1579(0.1578)
06:33:32 01-24 [epoch 62 train] [step 193/193] [eta 0:00:00]: lr 2.9481e-04 | loss 0.1585(0.1603)
06:33:35 01-24 [epoch 62 val]: Accuracy 75.0000 | Loss 0.6843
06:33:36 01-24 [epoch 63 train] [step 1/193] [eta 0:00:21]: lr 2.9415e-04 | loss 0.1565(0.1600)
06:33:37 01-24 [epoch 63 train] [step 50/193] [eta 0:00:15]: lr 2.6159e-04 | loss 0.1670(0.1670)
06:33:39 01-24 [epoch 63 train] [step 100/193] [eta 0:00:02]: lr 2.6159e-04 | loss 0.1614(0.1557)
06:33:40 01-24 [epoch 63 train] [step 150/193] [eta 0:00:01]: lr 2.6159e-04 | loss 0.1595(0.1558)
06:33:42 01-24 [epoch 63 train] [step 193/193] [eta 0:00:00]: lr 2.6159e-04 | loss 0.1626(0.1709)
06:33:44 01-24 [epoch 63 val]: Accuracy 77.6042 | Loss 0.6157
06:33:45 01-24 [epoch 64 train] [step 1/193] [eta 0:00:20]: lr 2.6095e-04 | loss 0.1564(0.1709)
06:33:47 01-24 [epoch 64 train] [step 50/193] [eta 0:00:14]: lr 2.2992e-04 | loss 0.1920(0.1920)
06:34:11 01-24 [epoch 64 train] [step 100/193] [eta 0:00:03]: lr 2.2992e-04 | loss 0.1746(0.1572)
06:33:50 01-24 [epoch 64 train] [step 150/193] [eta 0:00:01]: lr 2.2992e-04 | loss 0.1686(0.1565)
06:33:52 01-24 [epoch 64 train] [step 193/193] [eta 0:00:00]: lr 2.2992e-04 | loss 0.1722(0.1811)
06:33:56 01-24 [epoch 64 val]: Accuracy 73.4375 | Loss 0.6969
06:34:22 01-24 [epoch 65 train] [step 1/193] [eta 0:00:33]: lr 2.2933e-04 | loss 0.1539(0.1807)
06:34:00 01-24 [epoch 65 train] [step 50/193] [eta 0:00:24]: lr 2.0002e-04 | loss 0.2079(0.2079)
06:34:03 01-24 [epoch 65 train] [step 100/193] [eta 0:00:05]: lr 2.0002e-04 | loss 0.1887(0.1695)
06:34:05 01-24 [epoch 65 train] [step 150/193] [eta 0:00:01]: lr 2.0002e-04 | loss 0.1841(0.1748)
06:34:06 01-24 [epoch 65 train] [step 193/193] [eta 0:00:00]: lr 2.0002e-04 | loss 0.1784(0.1582)
06:34:09 01-24 [epoch 65 val]: Accuracy 77.6042 | Loss 0.6266
06:34:10 01-24 [epoch 66 train] [step 1/193] [eta 0:00:19]: lr 1.9946e-04 | loss 0.1624(0.1584)
06:34:12 01-24 [epoch 66 train] [step 50/193] [eta 0:00:16]: lr 1.7205e-04 | loss 0.1625(0.1625)
06:34:13 01-24 [epoch 66 train] [step 100/193] [eta 0:00:02]: lr 1.7205e-04 | loss 0.1620(0.1615)
06:34:15 01-24 [epoch 66 train] [step 150/193] [eta 0:00:01]: lr 1.7205e-04 | loss 0.1635(0.1666)
06:34:16 01-24 [epoch 66 train] [step 193/193] [eta 0:00:00]: lr 1.7205e-04 | loss 0.1699(0.1869)
06:34:19 01-24 [epoch 66 val]: Accuracy 75.5208 | Loss 0.5975
06:34:20 01-24 [epoch 67 train] [step 1/193] [eta 0:00:22]: lr 1.7154e-04 | loss 0.1581(0.1870)
06:34:22 01-24 [epoch 67 train] [step 50/193] [eta 0:00:16]: lr 1.4620e-04 | loss 0.1546(0.1546)
06:34:23 01-24 [epoch 67 train] [step 100/193] [eta 0:00:02]: lr 1.4620e-04 | loss 0.1582(0.1618)
06:34:25 01-24 [epoch 67 train] [step 150/193] [eta 0:00:01]: lr 1.4620e-04 | loss 0.1579(0.1574)
06:34:28 01-24 [epoch 67 train] [step 193/193] [eta 0:00:00]: lr 1.4620e-04 | loss 0.1578(0.1575)
06:34:31 01-24 [epoch 67 val]: Accuracy 76.0417 | Loss 0.6425
06:34:32 01-24 [epoch 68 train] [step 1/193] [eta 0:00:29]: lr 1.4573e-04 | loss 0.1580(0.1572)
06:34:57 01-24 [epoch 68 train] [step 50/193] [eta 0:00:16]: lr 1.2263e-04 | loss 0.1673(0.1673)
06:34:36 01-24 [epoch 68 train] [step 100/193] [eta 0:00:05]: lr 1.2263e-04 | loss 0.1685(0.1696)
06:34:39 01-24 [epoch 68 train] [step 150/193] [eta 0:00:02]: lr 1.2263e-04 | loss 0.1666(0.1628)
06:34:41 01-24 [epoch 68 train] [step 193/193] [eta 0:00:00]: lr 1.2263e-04 | loss 0.1640(0.1553)
06:34:44 01-24 [epoch 68 val]: Accuracy 79.6875 | Loss 0.5883
06:34:45 01-24 [epoch 69 train] [step 1/193] [eta 0:00:25]: lr 1.2220e-04 | loss 0.1533(0.1549)
06:34:47 01-24 [epoch 69 train] [step 50/193] [eta 0:00:16]: lr 1.0147e-04 | loss 0.1553(0.1553)
06:34:48 01-24 [epoch 69 train] [step 100/193] [eta 0:00:02]: lr 1.0147e-04 | loss 0.1620(0.1688)
06:34:50 01-24 [epoch 69 train] [step 150/193] [eta 0:00:01]: lr 1.0147e-04 | loss 0.1619(0.1616)
06:34:51 01-24 [epoch 69 train] [step 193/193] [eta 0:00:00]: lr 1.0147e-04 | loss 0.1672(0.1813)
06:34:54 01-24 [epoch 69 val]: Accuracy 76.0417 | Loss 0.6262
06:34:55 01-24 [epoch 70 train] [step 1/193] [eta 0:00:22]: lr 1.0110e-04 | loss 0.1533(0.1813)
06:34:57 01-24 [epoch 70 train] [step 50/193] [eta 0:00:16]: lr 8.2862e-05 | loss 0.1646(0.1646)
06:34:58 01-24 [epoch 70 train] [step 100/193] [eta 0:00:03]: lr 8.2862e-05 | loss 0.1635(0.1624)
06:35:02 01-24 [epoch 70 train] [step 150/193] [eta 0:00:02]: lr 8.2862e-05 | loss 0.1651(0.1682)
06:35:03 01-24 [epoch 70 train] [step 193/193] [eta 0:00:00]: lr 8.2862e-05 | loss 0.1680(0.1752)
06:35:06 01-24 [epoch 70 val]: Accuracy 78.6458 | Loss 0.6196
06:35:07 01-24 [epoch 71 train] [step 1/193] [eta 0:00:22]: lr 8.2543e-05 | loss 0.1539(0.1752)
06:35:09 01-24 [epoch 71 train] [step 50/193] [eta 0:00:17]: lr 6.6918e-05 | loss 0.1570(0.1570)
06:35:10 01-24 [epoch 71 train] [step 100/193] [eta 0:00:02]: lr 6.6918e-05 | loss 0.1594(0.1619)
06:35:13 01-24 [epoch 71 train] [step 150/193] [eta 0:00:02]: lr 6.6918e-05 | loss 0.1821(0.2273)
06:35:16 01-24 [epoch 71 train] [step 193/193] [eta 0:00:00]: lr 6.6918e-05 | loss 0.1769(0.1636)
06:35:21 01-24 [epoch 71 val]: Accuracy 77.0833 | Loss 0.6499
06:35:22 01-24 [epoch 72 train] [step 1/193] [eta 0:00:36]: lr 6.6655e-05 | loss 0.1534(0.1635)
06:35:23 01-24 [epoch 72 train] [step 50/193] [eta 0:00:20]: lr 5.3738e-05 | loss 0.1641(0.1641)
06:35:25 01-24 [epoch 72 train] [step 100/193] [eta 0:00:02]: lr 5.3738e-05 | loss 0.1629(0.1617)
06:35:26 01-24 [epoch 72 train] [step 150/193] [eta 0:00:01]: lr 5.3738e-05 | loss 0.1612(0.1579)
06:35:28 01-24 [epoch 72 train] [step 193/193] [eta 0:00:00]: lr 5.3738e-05 | loss 0.1601(0.1560)
06:35:31 01-24 [epoch 72 val]: Accuracy 78.6458 | Loss 0.6089
06:35:32 01-24 [epoch 73 train] [step 1/193] [eta 0:00:21]: lr 5.3531e-05 | loss 0.1653(0.1562)
06:35:34 01-24 [epoch 73 train] [step 50/193] [eta 0:00:18]: lr 4.3401e-05 | loss 0.1602(0.1602)
06:35:36 01-24 [epoch 73 train] [step 100/193] [eta 0:00:03]: lr 4.3401e-05 | loss 0.1594(0.1586)
06:35:38 01-24 [epoch 73 train] [step 150/193] [eta 0:00:01]: lr 4.3401e-05 | loss 0.1605(0.1627)
06:35:39 01-24 [epoch 73 train] [step 193/193] [eta 0:00:00]: lr 4.3401e-05 | loss 0.1599(0.1579)
06:35:42 01-24 [epoch 73 val]: Accuracy 77.0833 | Loss 0.6832
06:35:43 01-24 [epoch 74 train] [step 1/193] [eta 0:00:23]: lr 4.3252e-05 | loss 0.1538(0.1573)
06:35:45 01-24 [epoch 74 train] [step 50/193] [eta 0:00:16]: lr 3.5971e-05 | loss 0.1602(0.1602)
06:35:47 01-24 [epoch 74 train] [step 100/193] [eta 0:00:03]: lr 3.5971e-05 | loss 0.1722(0.1842)
06:35:48 01-24 [epoch 74 train] [step 150/193] [eta 0:00:01]: lr 3.5971e-05 | loss 0.1697(0.1646)
06:35:51 01-24 [epoch 74 train] [step 193/193] [eta 0:00:00]: lr 3.5971e-05 | loss 0.1667(0.1583)
06:35:57 01-24 [epoch 74 val]: Accuracy 76.5625 | Loss 0.6262
06:35:58 01-24 [epoch 75 train] [step 1/193] [eta 0:00:37]: lr 3.5882e-05 | loss 0.1533(0.1582)
06:35:59 01-24 [epoch 75 train] [step 50/193] [eta 0:00:24]: lr 3.1495e-05 | loss 0.1595(0.1595)
06:36:01 01-24 [epoch 75 train] [step 100/193] [eta 0:00:02]: lr 3.1495e-05 | loss 0.1787(0.1978)
06:36:02 01-24 [epoch 75 train] [step 150/193] [eta 0:00:01]: lr 3.1495e-05 | loss 0.1715(0.1573)
06:36:04 01-24 [epoch 75 train] [step 193/193] [eta 0:00:00]: lr 3.1495e-05 | loss 0.1701(0.1641)
06:36:09 01-24 [epoch 75 val]: Accuracy 77.0833 | Loss 0.6221
